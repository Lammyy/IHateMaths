\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{gelman}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{s-intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Aims}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Context}{1}{section.1.2}}
\citation{ADVVI}
\citation{kingma}
\citation{mescheder}
\citation{sugiyama}
\citation{huszar}
\citation{kingma}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{3}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Structure}{4}{section.1.4}}
\citation{DeepLearning}
\citation{neuroplast}
\citation{neuroplast}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 2}Background on Neural Networks}{6}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{6}{section.2.1}}
\citation{DeepLearning}
\citation{universal,cybenko}
\citation{mnist}
\citation{neuralstat}
\citation{haykin}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Individual Node Structure}{7}{section.2.2}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example structure of an individual node function with 3 inputs, labelled as $\bm  {x}=[x_0\hskip 1em\relax x_1\hskip 1em\relax x_2\hskip 1em\relax x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm  {\theta }=[\theta _0\hskip 1em\relax \theta _1\hskip 1em\relax \theta _2\hskip 1em\relax \theta _3]$.\relax }}{8}{figure.caption.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{8}{section.2.3}}
\citation{snyman}
\citation{wu}
\citation{cybenko}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation Function Plots\relax }}{9}{figure.caption.7}}
\citation{neuralstat}
\citation{DeepLearning}
\citation{sparse}
\citation{kolen}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Structure}{10}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example Neural Network Structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in Section 2.2: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm  {\Theta }$ denotes the weights of the network.\relax }}{11}{figure.caption.8}}
\citation{bishop}
\citation{xavier}
\citation{goodman}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Weight Initialisation}{13}{section.2.5}}
\citation{DeepLearning}
\citation{optimneural}
\citation{optim}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimisation}{14}{section.2.6}}
\citation{floudas}
\citation{batch,optimneural}
\citation{bengio}
\citation{optimneural}
\citation{adam}
\citation{backprop}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Back-Propagation}{16}{section.2.7}}
\citation{DeepLearning}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Back-Propagation Algorithm\relax }}{18}{algocf.1}}
\citation{gelman}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 3}Variational Inference}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Context}{19}{section.3.1}}
\citation{blei}
\citation{KL}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The KL Divergence}{20}{section.3.2}}
\newlabel{lemma:3.2.5}{{3.2.5}{21}{}{theorem.3.2.5}{}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Introduction to Variational Inference}{22}{section.3.3}}
\newlabel{eqn:3.3.1}{{3.3.1}{22}{Introduction to Variational Inference}{equation.3.3.1}{}}
\newlabel{3.3.2}{{3.3.2}{22}{Introduction to Variational Inference}{equation.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Derivation of the ELBO}{22}{section.3.4}}
\citation{pattern}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Mean-Field Variational Family}{24}{section.3.5}}
\citation{pattern}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Coordinate Ascent Variational Inference (CAVI)\relax }}{26}{algocf.2}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Example: Bayesian Mixture of Gaussians}{27}{section.3.6}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces CAVI Algorithm for Bayesian mixture of Gaussians\relax }}{30}{algocf.3}}
\citation{ADVVI}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Amortized Inference}{31}{section.3.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This is a DAG representing Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm  {\phi ^{(i)}}$ corresponding to each latent variable point $\bm  {z}^{(i)}$ has to be found. $\bm  {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }}{31}{figure.caption.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm  {\phi }$, and each data point $\bm  {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm  {\phi }}(\bm  {z}|\bm  {x})$ to find $\bm  {z}^{(i)}$.\relax }}{31}{figure.caption.12}}
\citation{kingma}
\citation{vae}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Example: Variational Autoencoder}{33}{section.3.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm  {x}$ is passed through the encoder $q_\phi (\bm  {z}|\bm  {x})$ to produce a mean vector $\bm  {\mu }$ and a variance vector $\bm  {\sigma }^2$. Random noise $\bm  {\epsilon }_1$ is sampled from $\mathcal  {N}(0,I_{M\times M})$ and transformed to generate $\bm  {z}$: $\bm  {z}=\bm  {\mu }+\bm  {\epsilon }\cdot \bm  {\sigma }^2$. This latent variable $\bm  {z}$ is passed through the decoder $p_\theta (\bm  {x}|\bm  {z})$ to reconstruct the data point as $\mathaccentV {tilde}07E{\bm  {x}}$.\relax }}{34}{figure.caption.13}}
\citation{mescheder}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip To generate new data $\mathaccentV {tilde}07E{\bm  {x}}$ similar to existing data $\bm  {x}$, we sample latent variable $\bm  {z}$ from the prior distribution $p(\bm  {z})$ and pass it through the decoder $p_\theta (\bm  {x}|\bm  {z})$.\relax }}{35}{figure.caption.14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Problems with Implicit Distributions}{35}{section.3.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Implicit Prior and/or Variational Posterior}{35}{subsection.3.9.1}}
\citation{ali}
\citation{tran}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\epsilon _1$ according to the mean vector $\bm  {\mu }$ and variance vector $\bm  {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal  {G}_\phi (\epsilon _1;\bm  {x})$ directly as an additional input. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ has the same explicit representation as in Figures 3.3 and 3.4.\relax }}{36}{figure.caption.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Implicit Likelihood}{36}{subsection.3.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts a variation of ``Adversarial Variational Bayes", in which noise is additionally added to the input of the generator of likelihood distribution samples $G_\theta (\epsilon _2;\bm  {x})$. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ is therefore implicit and consequently, its corresponding term in the optimisation problem cannot be evaluated.\relax }}{37}{figure.caption.16}}
\citation{tiao}
\citation{sugiyama,mohamed}
\citation{huszar}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 4}Density Ratio Estimation}{39}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Class Probability Estimation}{39}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Derivation}{39}{subsection.4.1.1}}
\citation{sugiyama}
\citation{gan}
\citation{pattern}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Prior-Contrastive Algorithm}{41}{subsection.4.1.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Prior-Contrastive Class Probability Estimation\relax }}{43}{algocf.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Joint-Contrastive Algorithm}{44}{subsection.4.1.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Joint-Contrastive Class Probability Estimation\relax }}{45}{algocf.5}}
\citation{nguyen}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divergence Minimisation}{46}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation}{46}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Prior-Contrastive Algorithm}{48}{subsection.4.2.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Prior-Contrastive Divergence Minimisation\relax }}{49}{algocf.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Joint-Contrastive Algorithm}{50}{subsection.4.2.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Joint-Contrastive Divergence Minimisation\relax }}{51}{algocf.7}}
\citation{tiao}
\citation{JS}
\citation{gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Alternative Derivation of Class Probability Estimation}{52}{subsection.4.2.4}}
\citation{huszar}
\citation{explain}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 5}Initial Experiment - Inference, ``Sprinkler" Example}{55}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Problem Context}{55}{section.5.1}}
\citation{gan}
\citation{relu1,relu2}
\citation{gan}
\citation{relu1,relu2}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly multimodal and unusually shaped. Clearly a flexible model for the posterior distribution is required, as a typical multivariate Gaussian model would fail to capture these features.\relax }}{56}{figure.caption.21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Program Structure}{56}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\epsilon })$ used in the ``Continuous Sprinkler" experiment. The number inside the node indicates how many nodes the layer has, and the text above the node describes the activation function. Recall that the input layer does not have an activation function, and Rectified Linear Units (ReLU) are used for most of the hidden layers due to their many advantages. In the Concat. layer, the two input vectors are concatenated and there is no activation function. We do not use an activation function for the output layer as $q_\phi (z|x)\in \mathbb  {R}$.\relax }}{57}{figure.caption.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in Figure 5.2. When the estimator takes the parametrisation of a discriminator $D_\alpha (z,x)\simeq \frac  {q(z,x)}{q(z,x)+p(z,x)}$, a sigmoid activation function is used for the output layer \citep  {gan}. A ReLU output layer is used when the estimator output is the direct density ratio $r_\alpha (z,x)\simeq \frac  {q(z,x)}{p(z,x)}$ \citep  {relu1, relu2}.\relax }}{57}{figure.caption.23}}
\citation{DeepLearning}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Sprinkler Prior-Contrastive Algorithm\relax }}{59}{algocf.8}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces Sprinkler Joint-Contrastive Algorithm\relax }}{60}{algocf.9}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{61}{section.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \relax }}{61}{table.caption.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Initial tests show that the variational posterior reaches optimality at an average KL divergence of about 1.325: as seen in sub-figure (a), this corresponds to outputs similar to the true posterior in sub-figure (c). The posterior output in sub-figure (b) with a larger average KL divergence of 1.3963 is less flexible and only uni-modal for $x=50$.\relax }}{61}{figure.caption.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Average KL Divergences (KDE Estimate)\relax }}{62}{figure.caption.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Estimator Losses\relax }}{62}{figure.caption.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces NELBOs\relax }}{62}{figure.caption.30}}
\citation{sparse}
\citation{leaky}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 6}Activation Function Experiment}{64}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Theory}{64}{section.6.1}}
\citation{nowozin}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Experiment Outline}{66}{section.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{66}{section.6.3}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \relax }}{66}{table.caption.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Average KL Divergences (KDE Estimate)\relax }}{67}{figure.caption.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Estimator Losses\relax }}{67}{figure.caption.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces NELBOs\relax }}{67}{figure.caption.34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 7}Algorithm Analysis}{69}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{69}{section.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Algorithm Generalisation}{70}{section.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Reverse KL Divergence}{70}{subsection.7.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}GAN Divergence}{70}{subsection.7.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Optimization Algorithms}{71}{section.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Prior-Contrastive}{71}{subsection.7.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Joint-Contrastive}{72}{subsection.7.3.2}}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 8}Comparing Optimal Estimators}{73}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Theory}{73}{section.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Experiment Outline}{73}{section.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Results}{74}{section.8.3}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Prior-Contrastive Results\relax }}{74}{table.caption.35}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces Joint-Contrastive Results\relax }}{74}{table.caption.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{76}{figure.caption.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{76}{figure.caption.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Prior-Contrastive NELBO\relax }}{76}{figure.caption.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Joint-Contrastive Average KL Divergence\relax }}{77}{figure.caption.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Joint-Contrastive Estimator Loss\relax }}{77}{figure.caption.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Joint-Contrastive NELBO\relax }}{77}{figure.caption.42}}
\citation{lecun}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 9}Comparing Undertrained Estimators}{78}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Theory}{78}{section.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Estimator Bounds}{78}{subsection.9.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}First and Second Derivatives of Estimator Loss Functions}{79}{subsection.9.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we have plotted the estimator output against its loss functional. The same x and y-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }}{80}{figure.caption.43}}
\citation{lecun}
\citation{nowozin}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.3}Displacement of Estimator Optimal Values}{82}{subsection.9.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Experiment Outline}{84}{section.9.2}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Results}{84}{section.9.3}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Prior-Contrastive Results\relax }}{85}{table.caption.44}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Joint-Contrastive Results\relax }}{85}{table.caption.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{86}{figure.caption.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{86}{figure.caption.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Prior-Contrastive NELBO\relax }}{86}{figure.caption.48}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Joint-Contrastive Average KL Divergence\relax }}{87}{figure.caption.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Joint-Contrastive Estimator Loss\relax }}{87}{figure.caption.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Joint-Contrastive NELBO\relax }}{87}{figure.caption.51}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 10}Data Generation - (MNIST image generation)}{88}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Experiment Outline}{88}{section.10.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Samples from the MNIST Dataset\relax }}{88}{figure.caption.52}}
\citation{nowozin,bgan}
\citation{nowozin,bgan}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\epsilon })$ used in the MNIST data generation experiment. The notation used in the figure is the same as in Figures 5.2 and 5.3, and overall the network structure is very similar except significantly more nodes are used, suiting the higher dimensionality of the data. The dimensionality of latent variable $z$ is either 2 or 10 depending on the experimental setting. We have arbitrarily chosen the number of random noise inputs $\epsilon $ to be 4.\relax }}{89}{figure.caption.53}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This is a diagram of the decoder network used in the MNIST data generation experiment. A sigmoid output layer is used to map the network output to $(0,1)$, suiting the grayscale nature of the data. The amount of effective hidden layers is the same as in the encoder network, but there are significantly more nodes in each layer. This is because the objective of the program is to output relatively high dimensional image data. Although this problem involves image analysis, due to the relatively small image size we refrain from using convolutional layers. This is consistent with other similar MNIST experiments \citep  {nowozin, bgan}.\relax }}{89}{figure.caption.54}}
\citation{nowozin,bgan,tiao}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the estimator network for the MNIST data generation experiment. Depending on the estimator parametrisation, the output activation function is either sigmoid for the class probability estimator $D_\alpha (z,x)\simeq \frac  {q_\phi (z|x)}{q_\phi (z|x)+p(z)}$, exponential for the direct ratio estimator $r_\alpha (z,x)\simeq \frac  {q_\phi (z|x)}{p(z)}$ or linear for the direct log ratio estimator $T_\alpha (z,x)\simeq \qopname  \relax o{log}\frac  {q_\phi (z|x)}{p(z)}$.\relax }}{90}{figure.caption.55}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder used in the MNIST data generation experiment. Note that there is no random noise added to the decoder.\relax }}{90}{figure.caption.56}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Low Dimensional Experiment Results}{91}{section.10.2}}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Reconstruction Errors for the Low Dimensional MNIST Generation Experiment\relax }}{91}{table.caption.57}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In the above average KL divergence plots, the only noticeable feature is that the direct log ratio estimator in sub-figure (a) consistently has a higher reconstruction error for most of the iterations.\relax }}{92}{figure.caption.58}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Noting the scale of the estimator loss plot in sub-figure (b), it is evident that the reverse KL divergence leads to unstable estimator training. Unlike the estimators plotted in Figure 9.3 (b), these estimators don't appear to stabilise after a certain period. Sub-figure (a) shows that the direct log ratio estimator loss is consistently higher than the other two estimators, correlating with its poorer reconstruction.\relax }}{92}{figure.caption.59}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}High Dimensional Experiment Results}{92}{section.10.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Despite the apparent instability of the estimators trained with the reverse KL divergence, the NELBO plot in sub-figure (b) is relatively consistent, leading to a smooth convergence as plotted in Figure 10.6 (b). The high estimator losses shown in Figure 10.7 (a) correspond to an overestimated NELBO as depicted in sub-figure (a).\relax }}{93}{figure.caption.60}}
\@writefile{lot}{\contentsline {table}{\numberline {10.2}{\ignorespaces Reconstruction Errors for the High Dimensional MNIST Generation Experiment\relax }}{94}{table.caption.61}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This plot shows that the reconstruction error associated with the reverse KL divergence is consistently higher for the majority of the program runtime.\relax }}{94}{figure.caption.62}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The estimator loss corresponding to the reverse KL divergence appears to be extremely unstable for the entire program runtime, spiking to values exceeding $10^{14}$. On the other hand, the GAN divergence estimator loss appears to be relatively stable.\relax }}{95}{figure.caption.63}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The instability of the reverse KL divergence estimator loss propagates to its NELBO estimation, which appears to fluctuate wildly in comparison to the GAN divergence's loss. This correlates to the superiority of the GAN divergence in this particular experiment. Again recalling Figures 9.3 and 9.6 (b), estimators formulated by the reverse KL divergence experience initial instability, but have more accurate density ratio estimation when stable. It is possible that the estimator in this experiment has not stabilised, leading to poorer network convergence. This may be due to the increased complexity of the distributions associated with the MNIST dataset, as opposed to the simplicity of the previous ``Sprinkler" problem.\relax }}{95}{figure.caption.64}}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 11}Conclusion and Further Research}{96}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Further Research}{96}{section.11.1}}
\citation{bgan}
\citation{ali}
\citation{wang}
\citation{vincent}
\bibstyle{apalike}
\bibdata{bible}
\bibcite{bengio}{{1}{2012}{{Bengio}}{{}}}
\bibcite{bishop}{{2}{1995}{{Bishop}}{{}}}
\bibcite{pattern}{{3}{2006}{{Bishop}}{{}}}
\bibcite{blei}{{4}{2017}{{Blei et~al.}}{{}}}
\bibcite{optim}{{5}{2004}{{Boyd and Vandenberghe}}{{}}}
\bibcite{neuralstat}{{6}{1994}{{Cheng and Titterington}}{{}}}
\bibcite{cybenko}{{7}{1989}{{Cybenko}}{{}}}
\bibcite{vae}{{8}{2016}{{Doersch}}{{}}}
\bibcite{ali}{{9}{2016}{{{Dumoulin} et~al.}}{{}}}
\bibcite{backprop}{{10}{1986}{{E.~Rumelhart et~al.}}{{}}}
\bibcite{floudas}{{11}{2005}{{Floudas}}{{}}}
\bibcite{JS}{{12}{2004}{{Fuglede and Topsoe}}{{}}}
\bibcite{gelman}{{13}{2004}{{Gelman et~al.}}{{}}}
\bibcite{xavier}{{14}{2010}{{Glorot and Bengio}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{98}{section.11.1}}
\bibcite{sparse}{{15}{2011}{{Glorot et~al.}}{{}}}
\bibcite{DeepLearning}{{16}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{gan}{{17}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{goodman}{{18}{1960}{{Goodman}}{{}}}
\bibcite{haykin}{{19}{1998}{{Haykin}}{{}}}
\bibcite{universal}{{20}{1991}{{Hornik}}{{}}}
\bibcite{huszar}{{21}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{adam}{{22}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kingma}{{23}{2013}{{{Kingma} and {Welling}}}{{}}}
\bibcite{kolen}{{24}{2001}{{Kolen and Kremer}}{{}}}
\bibcite{KL}{{25}{1959}{{Kullback}}{{}}}
\bibcite{lecun}{{26}{2012}{{LeCun et~al.}}{{}}}
\bibcite{batch}{{27}{2014}{{Li et~al.}}{{}}}
\bibcite{relu2}{{28}{2017}{{Liu et~al.}}{{}}}
\bibcite{leaky}{{29}{2013}{{Maas}}{{}}}
\bibcite{mescheder}{{30}{2017}{{Mescheder et~al.}}{{}}}
\bibcite{mohamed}{{31}{2016}{{{Mohamed} and {Lakshminarayanan}}}{{}}}
\bibcite{relu1}{{32}{2015}{{Nam and Sugiyama}}{{}}}
\bibcite{nguyen}{{33}{2010}{{Nguyen et~al.}}{{}}}
\bibcite{nowozin}{{34}{2016}{{Nowozin et~al.}}{{}}}
\bibcite{explain}{{35}{1993}{{P.~Wellman and Henrion}}{{}}}
\bibcite{optimneural}{{36}{2016}{{Ruder}}{{}}}
\bibcite{mnist}{{37}{2003}{{Simard et~al.}}{{}}}
\bibcite{snyman}{{38}{2005}{{Snyman}}{{}}}
\bibcite{sugiyama}{{39}{2012}{{Sugiyama et~al.}}{{}}}
\bibcite{tiao}{{40}{2018}{{Tiao et~al.}}{{}}}
\bibcite{tran}{{41}{2017}{{{Tran} et~al.}}{{}}}
\bibcite{bgan}{{42}{2016}{{{Uehara} et~al.}}{{}}}
\bibcite{vincent}{{43}{2008}{{Vincent et~al.}}{{}}}
\bibcite{wang}{{44}{2009}{{Wang et~al.}}{{}}}
\bibcite{wu}{{45}{2009}{{Wu}}{{}}}
\bibcite{ADVVI}{{46}{2017}{{Zhang et~al.}}{{}}}
\bibcite{neuroplast}{{47}{1992}{{Zilles}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix A}Kernel Density Estimation}{102}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
