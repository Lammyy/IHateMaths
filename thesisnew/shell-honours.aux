\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{gelman}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Aims}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Context}{1}{section.1.2}}
\citation{ADVVI}
\citation{kingma}
\citation{mescheder}
\citation{sugiyama}
\citation{huszar}
\citation{kingma}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{3}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Structure}{4}{section.1.4}}
\citation{DeepLearning}
\citation{neuroplast}
\citation{neuroplast}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 2}Background on Neural Networks}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch2}{{2}{5}{Background on Neural Networks}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{5}{section.2.1}}
\newlabel{sec:2.1}{{2.1}{5}{Motivation}{section.2.1}{}}
\citation{DeepLearning}
\citation{universal,cybenko}
\citation{mnist}
\citation{neuralstat}
\citation{haykin}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Individual Node Structure}{6}{section.2.2}}
\newlabel{sec:2.2}{{2.2}{6}{Individual Node Structure}{section.2.2}{}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example structure of an individual node function with 3 inputs, labelled as $\bm  {x}=[x_0\hskip 1em\relax x_1\hskip 1em\relax x_2\hskip 1em\relax x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm  {\theta }=[\theta _0\hskip 1em\relax \theta _1\hskip 1em\relax \theta _2\hskip 1em\relax \theta _3]$.\relax }}{7}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2.1}{{2.1}{7}{\small Example structure of an individual node function with 3 inputs, labelled as $\bm {x}=[x_0\quad x_1\quad x_2\quad x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm {\theta }=[\theta _0\quad \theta _1\quad \theta _2\quad \theta _3]$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{7}{section.2.3}}
\newlabel{sec:2.3}{{2.3}{7}{Activation Functions}{section.2.3}{}}
\citation{snyman}
\citation{wu}
\citation{cybenko}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation Function Plots\relax }}{8}{figure.caption.7}}
\newlabel{fig:2.2}{{2.2}{8}{Activation Function Plots\relax }{figure.caption.7}{}}
\citation{neuralstat}
\citation{DeepLearning}
\citation{sparse}
\citation{kolen}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Structure}{9}{section.2.4}}
\newlabel{sec:2.4}{{2.4}{9}{Neural Network Structure}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of a neural network structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in \autoref  {sec:2.2}: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm  {\Theta }$ denotes the weights of the network.\relax }}{10}{figure.caption.8}}
\newlabel{fig:2.3}{{2.3}{10}{\small Example of a neural network structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in \autoref {sec:2.2}: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm {\Theta }$ denotes the weights of the network.\relax }{figure.caption.8}{}}
\citation{bishop}
\citation{xavier}
\citation{goodman}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Weight Initialisation}{11}{section.2.5}}
\newlabel{sec:2.5}{{2.5}{11}{Weight Initialisation}{section.2.5}{}}
\citation{DeepLearning}
\citation{optimneural}
\citation{optim}
\citation{beck}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimisation}{12}{section.2.6}}
\newlabel{sec:2.6}{{2.6}{12}{Optimisation}{section.2.6}{}}
\citation{floudas}
\citation{batch,optimneural}
\citation{bengio}
\citation{optimneural}
\citation{adam}
\citation{backprop}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Back-Propagation}{14}{section.2.7}}
\newlabel{sec:2.7}{{2.7}{14}{Back-Propagation}{section.2.7}{}}
\citation{gelman}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 3}Variational Inference}{16}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch3}{{3}{16}{Variational Inference}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Context}{16}{section.3.1}}
\newlabel{sec:3.1}{{3.1}{16}{Context}{section.3.1}{}}
\citation{blei}
\citation{KL}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The KL Divergence}{17}{section.3.2}}
\newlabel{sec:3.2}{{3.2}{17}{The KL Divergence}{section.3.2}{}}
\newlabel{def:3.2.1}{{3.2.1}{17}{}{theorem.3.2.1}{}}
\newlabel{3.2.5}{{3.2.5}{18}{}{theorem.3.2.5}{}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Introduction to Variational Inference}{19}{section.3.3}}
\newlabel{sec:3.3}{{3.3}{19}{Introduction to Variational Inference}{section.3.3}{}}
\newlabel{eqn:3.3.1}{{3.3.1}{19}{Introduction to Variational Inference}{equation.3.3.1}{}}
\newlabel{eqn:3.3.2}{{3.3.2}{19}{Introduction to Variational Inference}{equation.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Derivation of the ELBO}{19}{section.3.4}}
\newlabel{sec:3.4}{{3.4}{19}{Derivation of the ELBO}{section.3.4}{}}
\citation{pattern}
\citation{blei}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Mean-Field Variational Family}{21}{section.3.5}}
\newlabel{sec:3.5}{{3.5}{21}{Mean-Field Variational Family}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Amortized Inference}{21}{section.3.6}}
\citation{ADVVI}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm  {\phi ^{(i)}}$ corresponding to each latent variable point $\bm  {z}^{(i)}$ has to be found. $\bm  {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }}{22}{figure.caption.9}}
\newlabel{fig:3.1}{{3.1}{22}{\small This DAG represents Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm {\phi ^{(i)}}$ corresponding to each latent variable point $\bm {z}^{(i)}$ has to be found. $\bm {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm  {\phi }$, and each data point $\bm  {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm  {\phi }}(\bm  {z}|\bm  {x})$ to find $\bm  {z}^{(i)}$.\relax }}{22}{figure.caption.10}}
\newlabel{fig:3.2}{{3.2}{22}{\small This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm {\phi }$, and each data point $\bm {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm {\phi }}(\bm {z}|\bm {x})$ to find $\bm {z}^{(i)}$.\relax }{figure.caption.10}{}}
\citation{kingma}
\citation{vae}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Example: Variational Autoencoder}{24}{section.3.7}}
\newlabel{sec:3.7}{{3.7}{24}{Example: Variational Autoencoder}{section.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm  {x}$ is passed through the encoder $q_\phi (\bm  {z}|\bm  {x})$ to produce a mean vector $\bm  {\mu }$ and a variance vector $\bm  {\sigma }^2$. Random noise $\bm  {\epsilon }_1$ is sampled from $\mathcal  {N}(0,I_{M\times M})$ and transformed to generate $\bm  {z}$: $\bm  {z}=\bm  {\mu }+\bm  {\epsilon }\cdot \bm  {\sigma }^2$. This latent variable $\bm  {z}$ is passed through the decoder $p_\theta (\bm  {x}|\bm  {z})$ to reconstruct the data point as $\mathaccentV {tilde}07E{\bm  {x}}$.\relax }}{25}{figure.caption.11}}
\newlabel{fig:3.3}{{3.3}{25}{\small A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm {x}$ is passed through the encoder $q_\phi (\bm {z}|\bm {x})$ to produce a mean vector $\bm {\mu }$ and a variance vector $\bm {\sigma }^2$. Random noise $\bm {\epsilon }_1$ is sampled from $\mathcal {N}(0,I_{M\times M})$ and transformed to generate $\bm {z}$: $\bm {z}=\bm {\mu }+\bm {\epsilon }\cdot \bm {\sigma }^2$. This latent variable $\bm {z}$ is passed through the decoder $p_\theta (\bm {x}|\bm {z})$ to reconstruct the data point as $\tilde {\bm {x}}$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip To generate new data $\mathaccentV {tilde}07E{\bm  {x}}$ similar to existing data $\bm  {x}$, we sample latent variable $\bm  {z}$ from the prior distribution $p(\bm  {z})$ and pass it through the decoder $p_\theta (\bm  {x}|\bm  {z})$.\relax }}{25}{figure.caption.12}}
\newlabel{fig:3.4}{{3.4}{25}{\small To generate new data $\tilde {\bm {x}}$ similar to existing data $\bm {x}$, we sample latent variable $\bm {z}$ from the prior distribution $p(\bm {z})$ and pass it through the decoder $p_\theta (\bm {x}|\bm {z})$.\relax }{figure.caption.12}{}}
\citation{mescheder}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Problems with Implicit Distributions}{26}{section.3.8}}
\newlabel{sec:3.8}{{3.8}{26}{Problems with Implicit Distributions}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Implicit Prior and/or Variational Posterior}{26}{subsection.3.8.1}}
\newlabel{sec:3.8.1}{{3.8.1}{26}{Implicit Prior and/or Variational Posterior}{subsection.3.8.1}{}}
\citation{ali}
\citation{tran}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\epsilon _1$ according to the mean vector $\bm  {\mu }$ and variance vector $\bm  {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal  {G}_\phi (\epsilon _1;\bm  {x})$ directly as an additional input. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ has the same explicit representation as in Figures \ref  {fig:3.3} and \ref  {fig:3.4}.\relax }}{27}{figure.caption.13}}
\newlabel{fig:3.5}{{3.5}{27}{\small This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\epsilon _1$ according to the mean vector $\bm {\mu }$ and variance vector $\bm {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal {G}_\phi (\epsilon _1;\bm {x})$ directly as an additional input. The likelihood distribution $p_\theta (\bm {x}|\bm {z})$ has the same explicit representation as in Figures \ref {fig:3.3} and \ref {fig:3.4}.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Implicit Likelihood}{27}{subsection.3.8.2}}
\newlabel{sec:3.8.2}{{3.8.2}{27}{Implicit Likelihood}{subsection.3.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts a variation of ``Adversarial Variational Bayes", in which noise is additionally added to the input of the generator of likelihood distribution samples $G_\theta (\epsilon _2;\bm  {x})$. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ is therefore implicit and consequently, its corresponding term in the optimisation problem cannot be evaluated.\relax }}{27}{figure.caption.14}}
\newlabel{fig:3.6}{{3.6}{27}{\small This diagram depicts a variation of ``Adversarial Variational Bayes", in which noise is additionally added to the input of the generator of likelihood distribution samples $G_\theta (\epsilon _2;\bm {x})$. The likelihood distribution $p_\theta (\bm {x}|\bm {z})$ is therefore implicit and consequently, its corresponding term in the optimisation problem cannot be evaluated.\relax }{figure.caption.14}{}}
\citation{tiao}
\citation{sugiyama,mohamed}
\citation{huszar}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 4}Density Ratio Estimation}{30}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch4}{{4}{30}{Density Ratio Estimation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Class Probability Estimation}{30}{section.4.1}}
\newlabel{sec:4.1}{{4.1}{30}{Class Probability Estimation}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Derivation}{30}{subsection.4.1.1}}
\newlabel{sec:4.1.1}{{4.1.1}{30}{Derivation}{subsection.4.1.1}{}}
\citation{sugiyama}
\citation{gan}
\newlabel{rem:4.1.2}{{4.1.2}{31}{}{theorem.4.1.2}{}}
\newlabel{rem:4.1.3}{{4.1.3}{31}{}{theorem.4.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Prior-Contrastive Algorithm}{32}{subsection.4.1.2}}
\newlabel{sec:4.1.2}{{4.1.2}{32}{Prior-Contrastive Algorithm}{subsection.4.1.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Prior-Contrastive Class Probability Estimation\relax }}{34}{algocf.1}}
\newlabel{alg:1}{{1}{34}{Prior-Contrastive Algorithm}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Joint-Contrastive Algorithm}{34}{subsection.4.1.3}}
\newlabel{sec:4.1.3}{{4.1.3}{34}{Joint-Contrastive Algorithm}{subsection.4.1.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Joint-Contrastive Class Probability Estimation\relax }}{36}{algocf.2}}
\newlabel{alg:2}{{2}{36}{Joint-Contrastive Algorithm}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divergence Minimisation}{36}{section.4.2}}
\newlabel{sec:4.2}{{4.2}{36}{Divergence Minimisation}{section.4.2}{}}
\citation{nguyen}
\citation{nguyen}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation}{37}{subsection.4.2.1}}
\newlabel{sec:4.2.1}{{4.2.1}{37}{Derivation}{subsection.4.2.1}{}}
\newlabel{4.2.1}{{4.2.1}{37}{}{theorem.4.2.1}{}}
\newlabel{rem:4.2.4}{{4.2.4}{38}{}{theorem.4.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Prior-Contrastive Algorithm}{38}{subsection.4.2.2}}
\newlabel{sec:4.2.2}{{4.2.2}{38}{Prior-Contrastive Algorithm}{subsection.4.2.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Prior-Contrastive Divergence Minimisation\relax }}{39}{algocf.3}}
\newlabel{alg:3}{{3}{39}{Prior-Contrastive Algorithm}{algocf.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Joint-Contrastive Algorithm}{40}{subsection.4.2.3}}
\newlabel{sec:4.2.3}{{4.2.3}{40}{Joint-Contrastive Algorithm}{subsection.4.2.3}{}}
\citation{tiao}
\citation{JS}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Joint-Contrastive Divergence Minimisation\relax }}{41}{algocf.4}}
\newlabel{alg:4}{{4}{41}{Joint-Contrastive Algorithm}{algocf.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Alternative Derivation of Class Probability Estimation}{41}{subsection.4.2.4}}
\newlabel{sec:4.2.4}{{4.2.4}{41}{Alternative Derivation of Class Probability Estimation}{subsection.4.2.4}{}}
\citation{gan}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 5}Algorithm Generalisation}{44}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch5}{{5}{44}{Algorithm Generalisation}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{44}{section.5.1}}
\newlabel{sec:5.1}{{5.1}{44}{Introduction}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm Generalisation}{45}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Reverse KL Divergence}{45}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}GAN Divergence}{45}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Optimization Algorithms}{46}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Prior-Contrastive}{46}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Joint-Contrastive}{47}{subsection.5.3.2}}
\citation{nowozin}
\citation{huszar}
\citation{explain}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 6}Comparing Optimal Estimators}{48}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch6}{{6}{48}{Comparing Optimal Estimators}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Theory}{48}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Problem Context}{48}{section.6.2}}
\citation{gan}
\citation{nowozin}
\citation{gan}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly multimodal and unusually shaped. Clearly a flexible model for the posterior distribution is required, as a typical multivariate Gaussian model would fail to capture these features.\relax }}{50}{figure.caption.19}}
\newlabel{fig:6.1}{{6.1}{50}{\small This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly multimodal and unusually shaped. Clearly a flexible model for the posterior distribution is required, as a typical multivariate Gaussian model would fail to capture these features.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Program Structure}{50}{section.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\epsilon })$ used in the ``Continuous Sprinkler" experiment. The number inside the node indicates how many nodes the layer has, and the text above the node describes the activation function. Recall that the input layer does not have an activation function, and Rectified Linear Units (ReLU) are used for most of the hidden layers due to their many advantages. In the Concat. layer, the two input vectors are concatenated and there is no activation function. We do not use an activation function for the output layer as $q_\phi (\bm  {z}|x)\in \mathbb  {R}$.\relax }}{50}{figure.caption.20}}
\newlabel{fig:6.2}{{6.2}{50}{\small This figure illustrates the structure of the generator network $\mathcal {G}(x,\bm {\epsilon })$ used in the ``Continuous Sprinkler" experiment. The number inside the node indicates how many nodes the layer has, and the text above the node describes the activation function. Recall that the input layer does not have an activation function, and Rectified Linear Units (ReLU) are used for most of the hidden layers due to their many advantages. In the Concat. layer, the two input vectors are concatenated and there is no activation function. We do not use an activation function for the output layer as $q_\phi (\bm {z}|x)\in \R $.\relax }{figure.caption.20}{}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in \autoref  {fig:6.2}. When the estimator takes the parametrisation of a discriminator $D_\alpha (z,x)$, a sigmoid output layer is used \citep  {gan}. An exponential activation is used for the output layer when the estimator output is the direct density ratio $r_\alpha (z,x)$, and a linear output is used for the direct log density ratio estimator $T_\alpha (z,x)$ \citep  {nowozin}.\relax }}{51}{figure.caption.21}}
\newlabel{fig:6.3}{{6.3}{51}{\small This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in \autoref {fig:6.2}. When the estimator takes the parametrisation of a discriminator $D_\alpha (z,x)$, a sigmoid output layer is used \citep {gan}. An exponential activation is used for the output layer when the estimator output is the direct density ratio $r_\alpha (z,x)$, and a linear output is used for the direct log density ratio estimator $T_\alpha (z,x)$ \citep {nowozin}.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Results}{52}{section.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we compare the true (unnormalised) posterior as plotted in \autoref  {fig:6.1} against variational posterior outputs corresponding to varying KL divergences. Sub-figures (b)-(3) were created by fitting a Gaussian kernel density estimation over 1000 posterior samples. Note that the variational posterior is relatively optimal at a `true' KL divergence of 1.3288, and that the output appears less flexible as the divergence increases. In sub-figure (e), the variational posterior fails to capture the bimodality of the true posterior.\relax }}{53}{figure.caption.22}}
\newlabel{fig:6.4}{{6.4}{53}{\small Above we compare the true (unnormalised) posterior as plotted in \autoref {fig:6.1} against variational posterior outputs corresponding to varying KL divergences. Sub-figures (b)-(3) were created by fitting a Gaussian kernel density estimation over 1000 posterior samples. Note that the variational posterior is relatively optimal at a `true' KL divergence of 1.3288, and that the output appears less flexible as the divergence increases. In sub-figure (e), the variational posterior fails to capture the bimodality of the true posterior.\relax }{figure.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This table presents the results from the prior-contrastive experiment. Since the programs reached optimality in this case, it is uncertain from those results whether the choice of $f$-divergence or estimator parametrisation impacts posterior convergence.\relax }}{54}{table.caption.23}}
\newlabel{tab:6.1}{{6.1}{54}{\small This table presents the results from the prior-contrastive experiment. Since the programs reached optimality in this case, it is uncertain from those results whether the choice of $f$-divergence or estimator parametrisation impacts posterior convergence.\relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip From these joint-contrastive results, it is evident that for each $f$-divergence, similar posterior convergence is associated with the different estimator parametrisations. As Nowozin's paper suggests, the GAN divergence demonstrates slower and more inconsistent results than the reverse KL divergence.\relax }}{54}{table.caption.24}}
\newlabel{tab:6.2}{{6.2}{54}{\small From these joint-contrastive results, it is evident that for each $f$-divergence, similar posterior convergence is associated with the different estimator parametrisations. As Nowozin's paper suggests, the GAN divergence demonstrates slower and more inconsistent results than the reverse KL divergence.\relax }{table.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These plots of the true KL divergence in the joint-contrastive context show near identical convergence for all estimators.\relax }}{54}{figure.caption.25}}
\newlabel{fig:6.5}{{6.5}{54}{\small These plots of the true KL divergence in the joint-contrastive context show near identical convergence for all estimators.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In sub-figure (a), the estimators have similar levels of stability, but the class probability estimator loss appears to be higher than the other two estimator parametrisations. All three estimators in sub-figure (b) have similar, increasingly unstable losses.\relax }}{55}{figure.caption.26}}
\newlabel{fig:6.6}{{6.6}{55}{\small In sub-figure (a), the estimators have similar levels of stability, but the class probability estimator loss appears to be higher than the other two estimator parametrisations. All three estimators in sub-figure (b) have similar, increasingly unstable losses.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These joint-contrastive $NELBO$ plots generally mirror the estimator loss trends depicted in \autoref  {fig:6.6}. Lower and more stable $NELBO$ estimations are associated with the reverse KL divergence, correlating with its superior posterior convergence. It is unclear why the $NELBO$ increases over the iterations when its minimisation theoretically leads to posterior convergence.\relax }}{55}{figure.caption.27}}
\newlabel{fig:6.7}{{6.7}{55}{\small These joint-contrastive $NELBO$ plots generally mirror the estimator loss trends depicted in \autoref {fig:6.6}. Lower and more stable $NELBO$ estimations are associated with the reverse KL divergence, correlating with its superior posterior convergence. It is unclear why the $NELBO$ increases over the iterations when its minimisation theoretically leads to posterior convergence.\relax }{figure.caption.27}{}}
\citation{lecun}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 7}Comparing Undertrained Estimators}{56}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch7}{{7}{56}{Comparing Undertrained Estimators}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Theory}{56}{section.7.1}}
\newlabel{sec:7.1}{{7.1}{56}{Theory}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Estimator Bounds}{56}{subsection.7.1.1}}
\citation{lecun}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}First and Second Derivatives of Estimator Loss Functions}{57}{subsection.7.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we have plotted the estimator output against its loss functional. The same x and y-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }}{58}{figure.caption.28}}
\newlabel{fig:7.1}{{7.1}{58}{\small Above we have plotted the estimator output against its loss functional. The same x and y-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }{figure.caption.28}{}}
\citation{nowozin}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Displacement of Estimator Optimal Values}{60}{subsection.7.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Experiment Outline}{61}{section.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{61}{section.7.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these prior-contrastive results, it is evident that using the reverse KL divergence to derive the lower bound for the estimator loss function leads to significantly higher posterior convergence, supporting our experimental results in \autoref  {ch6}.\relax }}{62}{table.caption.31}}
\newlabel{tab:7.1}{{7.1}{62}{\small In these prior-contrastive results, it is evident that using the reverse KL divergence to derive the lower bound for the estimator loss function leads to significantly higher posterior convergence, supporting our experimental results in \autoref {ch6}.\relax }{table.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the reduction in the true average KL divergence over the prior contrastive experiments' runtime. Although the direct ratio estimator plots initially experience a faster reduction, all three estimator plots meet at approximately the same KL divergence. It can be seen that the GAN divergence is associated with a faster initial drop than the reverse KL divergence, yet experiences lower overall posterior convergence.\relax }}{62}{figure.caption.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these prior-contrastive estimator loss plots, it is evident that the estimators bound by the reverse KL divergence experience initial fluctuations which eventually stabilise, whilst the GAN divergence is associated with higher stability. This is consistent with the observations made in Figure 7.2, and implies a trade-off between estimator stability and accuracy in the choice of $f$-divergence.\relax }}{63}{figure.caption.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The trends shown in the $NELBO$ plots above are generally consistent with the corresponding estimator loss plots. All of the estimators converge at approximately the same estimated $NELBO$, indicating that the effects of under-training the estimator are mostly experienced in the early program iterations.\relax }}{63}{figure.caption.34}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these joint-contrastive results, there is a significant difference between the three estimators when the reverse KL bound is used. However, when the GAN divergence is used to formulate the estimator loss function, only the direct log ratio estimator demonstrates significantly worse posterior convergence than the other two estimators.\relax }}{64}{table.caption.35}}
\newlabel{tab:7.2}{{7.2}{64}{\small In these joint-contrastive results, there is a significant difference between the three estimators when the reverse KL bound is used. However, when the GAN divergence is used to formulate the estimator loss function, only the direct log ratio estimator demonstrates significantly worse posterior convergence than the other two estimators.\relax }{table.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Similar to the true KL divergence plots in Figure 7.2, these joint-contrastive plots show that the estimators formulated with the GAN divergence experience faster initial convergence. However, this is unreliable as the reverse KL divergence experiments appear to initialise at a much higher true KL divergence.\relax }}{64}{figure.caption.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these estimator loss plots, we again see that the reverse KL divergence plot demonstrates initial estimator instability. This is in stark contrast to the relatively stable GAN divergence plot. From sub-figure (a), we note that the direct ratio estimator is more unstable and has a lower loss value than the other two estimators. This may be the result of an outlier experiment, as the direct ratio estimator results have the highest standard deviation of the GAN divergence experiments, yet it has similar posterior convergence to the more consistent class probability estimator.\relax }}{65}{figure.caption.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The joint-contrastive estimated $NELBO$ plots above generally have similar trends to the corresponding estimator plots. Here we note that the final $NELBO$ value of the reverse KL divergence experiments is lower than that of the GAN divergence. However, this may be due to its initial fluctuations delaying the start of the increasing $NELBO$ trend by approximately 250 iterations.\relax }}{65}{figure.caption.38}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 8}Autoencoder Experiment - (MNIST Dataset)}{66}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch8}{{8}{66}{Autoencoder Experiment - (MNIST Dataset)}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Experiment Outline}{66}{section.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Samples from the MNIST Dataset\relax }}{66}{figure.caption.39}}
\newlabel{fig:8.1}{{8.1}{66}{Samples from the MNIST Dataset\relax }{figure.caption.39}{}}
\citation{nowozin,bgan}
\citation{nowozin,bgan}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\varepsilon })$ used in the MNIST data generation experiment. The notation used in the figure is the same as in Figures 5.2 and 5.3, and overall the network structure is very similar except significantly more nodes are used, suiting the higher dimensionality of the data. The dimensionality of latent variable $z$ is either 2 or 10 depending on the experimental setting. We have arbitrarily chosen the number of random noise inputs $\varepsilon $ to be 4.\relax }}{67}{figure.caption.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This is a diagram of the decoder network used in the MNIST data generation experiment. A sigmoid output layer is used to map the network output to $(0,1)$, suiting the grayscale nature of the data. The amount of effective hidden layers is the same as in the encoder network, but there are significantly more nodes in each layer. This is because the objective of the program is to output relatively high dimensional image data. Although this problem involves image analysis, due to the relatively small image size we refrain from using convolutional layers. This is consistent with other similar MNIST experiments \citep  {nowozin, bgan}.\relax }}{67}{figure.caption.41}}
\citation{nowozin,bgan,tiao}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the estimator network for the MNIST data generation experiment. Depending on the estimator parametrisation, the output activation function is either sigmoid for the class probability estimator $D_\alpha (z,x)\simeq \frac  {q_\phi (z|x)}{q_\phi (z|x)+p(z)}$, exponential for the direct ratio estimator $r_\alpha (z,x)\simeq \frac  {q_\phi (z|x)}{p(z)}$ or linear for the direct log ratio estimator $T_\alpha (z,x)\simeq \qopname  \relax o{log}\frac  {q_\phi (z|x)}{p(z)}$.\relax }}{68}{figure.caption.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder used in the MNIST data generation experiment. Note that there is no random noise added to the decoder.\relax }}{69}{figure.caption.43}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Low Dimensional Experiment Results}{69}{section.8.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The low-dimensional MNIST autoencoder experiment results are tabulated above. The only notable observation is that the direct log ratio estimator trained with a GAN divergence leads to significantly higher reconstruction error. There is also a consistent but relatively insignificant trend in the estimator parametrisations: again the class probability estimator correlates with the lowest reconstruction errors, followed by the direct ratio estimator and the direct log ratio estimator. Additionally, the reverse KL divergence correlates to superior posterior convergence.\relax }}{69}{table.caption.44}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These figures exemplify data reconstruction corresponding to different reconstruction errors. The first two rows of each sub-figure depict example data input $x$, and the last two rows show the reconstructed output $\mathaccentV {tilde}07E{x}$.\relax }}{70}{figure.caption.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip As shown in the above average reconstruction error plots, the direct log ratio estimator in sub-figure (a) consistently has a higher reconstruction error.\relax }}{70}{figure.caption.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Noting the scale of the estimator loss plot in sub-figure (b), it is evident that the reverse KL divergence leads to unstable estimator training. Unlike the estimators plotted in Figure 7.3 (b), these estimators don't appear to stabilise after a certain period. Sub-figure (a) shows that the direct log ratio estimator loss is consistently higher than the other two estimators, correlating with its poorer reconstruction.\relax }}{71}{figure.caption.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Despite the apparent instability of the estimators trained with the reverse KL divergence, the $NELBO$ plot in sub-figure (b) is relatively consistent, leading to a smooth convergence as plotted in Figure 8.7 (b). The high direct log ratio estimator loss associated with the GAN divergence corresponds to a relatively large $NELBO$ as depicted in sub-figure (a).\relax }}{71}{figure.caption.48}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}High Dimensional Experiment Results}{71}{section.8.3}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In the high dimensional MNIST autoencoder experiment results, it is evident that the GAN divergence correlates with significantly lower reconstruction error mean and standard deviation, contradicting the superiority of the reverse KL divergence shown in previous experiments. An analysis of the plots in Figures 8.11-8.13 may explain this occurrence.\relax }}{72}{table.caption.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Similar to Figure 8.6, these figures exemplify data reconstruction corresponding to different reconstruction errors.\relax }}{73}{figure.caption.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This plot shows that the reconstruction error associated with the reverse KL divergence is consistently higher for the majority of the program runtime.\relax }}{73}{figure.caption.51}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The estimator loss corresponding to the reverse KL divergence appears to be extremely unstable for the entire program runtime, spiking to values exceeding $10^{14}$. On the other hand, the GAN divergence estimator loss appears to be relatively stable.\relax }}{74}{figure.caption.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The instability of the reverse KL divergence estimator loss propagates to its $NELBO$ estimation, which appears to fluctuate wildly. This correlates to the superiority of the GAN divergence in this particular experiment. Recalling the results in \autoref  {ch7}, estimators formulated by the reverse KL divergence experience initial instability, but have more accurate density ratio estimation when stable. It is possible that the estimator in this experiment has not stabilised, leading to poorer network convergence. This may be due to the increased complexity of the MNIST dataset distribution, as opposed to the simplicity of the previous ``Sprinkler" problem.\relax }}{74}{figure.caption.53}}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 9}Conclusion and Further Research}{75}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch9}{{9}{75}{Conclusion and Further Research}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Further Research}{75}{section.9.1}}
\citation{bgan}
\citation{ali}
\citation{wang}
\citation{vincent}
\citation{ais}
\bibstyle{apalike}
\bibdata{bible}
\bibcite{beck}{{1}{2014}{{Beck}}{{}}}
\bibcite{bengio}{{2}{2012}{{Bengio}}{{}}}
\bibcite{bishop}{{3}{1995}{{Bishop}}{{}}}
\bibcite{pattern}{{4}{2006}{{Bishop}}{{}}}
\bibcite{blei}{{5}{2017}{{Blei et~al.}}{{}}}
\bibcite{optim}{{6}{2004}{{Boyd and Vandenberghe}}{{}}}
\bibcite{neuralstat}{{7}{1994}{{Cheng and Titterington}}{{}}}
\bibcite{cybenko}{{8}{1989}{{Cybenko}}{{}}}
\bibcite{vae}{{9}{2016}{{Doersch}}{{}}}
\bibcite{ali}{{10}{2016}{{{Dumoulin} et~al.}}{{}}}
\bibcite{backprop}{{11}{1986}{{E.~Rumelhart et~al.}}{{}}}
\bibcite{floudas}{{12}{2005}{{Floudas}}{{}}}
\bibcite{JS}{{13}{2004}{{Fuglede and Topsoe}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{77}{section.9.1}}
\bibcite{gelman}{{14}{2004}{{Gelman et~al.}}{{}}}
\bibcite{xavier}{{15}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{sparse}{{16}{2011}{{Glorot et~al.}}{{}}}
\bibcite{DeepLearning}{{17}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{gan}{{18}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{goodman}{{19}{1960}{{Goodman}}{{}}}
\bibcite{haykin}{{20}{1998}{{Haykin}}{{}}}
\bibcite{universal}{{21}{1991}{{Hornik}}{{}}}
\bibcite{huszar}{{22}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{adam}{{23}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kingma}{{24}{2013}{{{Kingma} and {Welling}}}{{}}}
\bibcite{kolen}{{25}{2001}{{Kolen and Kremer}}{{}}}
\bibcite{KL}{{26}{1959}{{Kullback}}{{}}}
\bibcite{lecun}{{27}{2012}{{LeCun et~al.}}{{}}}
\bibcite{batch}{{28}{2014}{{Li et~al.}}{{}}}
\bibcite{mescheder}{{29}{2017}{{Mescheder et~al.}}{{}}}
\bibcite{mohamed}{{30}{2016}{{{Mohamed} and {Lakshminarayanan}}}{{}}}
\bibcite{nguyen}{{31}{2010}{{Nguyen et~al.}}{{}}}
\bibcite{nowozin}{{32}{2016}{{Nowozin et~al.}}{{}}}
\bibcite{explain}{{33}{1993}{{P.~Wellman and Henrion}}{{}}}
\bibcite{optimneural}{{34}{2016}{{Ruder}}{{}}}
\bibcite{mnist}{{35}{2003}{{Simard et~al.}}{{}}}
\bibcite{snyman}{{36}{2005}{{Snyman}}{{}}}
\bibcite{sugiyama}{{37}{2012}{{Sugiyama et~al.}}{{}}}
\bibcite{tiao}{{38}{2018}{{Tiao et~al.}}{{}}}
\bibcite{tran}{{39}{2017}{{{Tran} et~al.}}{{}}}
\bibcite{bgan}{{40}{2016}{{{Uehara} et~al.}}{{}}}
\bibcite{vincent}{{41}{2008}{{Vincent et~al.}}{{}}}
\bibcite{wang}{{42}{2009}{{Wang et~al.}}{{}}}
\bibcite{wu}{{43}{2009}{{Wu}}{{}}}
\bibcite{ais}{{44}{2016}{{Wu et~al.}}{{}}}
\bibcite{kde}{{45}{2012}{{{Zanin Zambom} and {Dias}}}{{}}}
\bibcite{ADVVI}{{46}{2017}{{Zhang et~al.}}{{}}}
\bibcite{neuroplast}{{47}{1992}{{Zilles}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix A}Proofs}{81}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:A}{{A}{81}{Proofs}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Proof of Proposition 2.6.2}{81}{appendix.A.1}}
\newlabel{app:A.1}{{A.1}{81}{Proof of Proposition 2.6.2}{appendix.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Lemma 3.7.1}{82}{appendix.A.2}}
\newlabel{app:A.2}{{A.2}{82}{Proof of Lemma 3.7.1}{appendix.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of Lemma 4.1.1}{82}{appendix.A.3}}
\newlabel{app:A.3}{{A.3}{82}{Proof of Lemma 4.1.1}{appendix.A.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Proof of Lemma 4.2.2}{83}{appendix.A.4}}
\newlabel{app:A.4}{{A.4}{83}{Proof of Lemma 4.2.2}{appendix.A.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Proof of Lemma 8.3.1}{83}{appendix.A.5}}
\newlabel{app:A.5}{{A.5}{83}{Proof of Lemma 8.3.1}{appendix.A.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix B}Algorithms}{84}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:B}{{B}{84}{Algorithms}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Back-Propagation Algorithm}{84}{appendix.B.1}}
\newlabel{app:B.1}{{B.1}{84}{Back-Propagation Algorithm}{appendix.B.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Back-Propagation Algorithm\relax }}{84}{algocf.5}}
\newlabel{alg:5}{{5}{84}{Back-Propagation Algorithm}{algocf.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Coordinate Ascent Variational Inference Algorithm}{85}{appendix.B.2}}
\newlabel{app:B.2}{{B.2}{85}{Coordinate Ascent Variational Inference Algorithm}{appendix.B.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Coordinate Ascent Variational Inference (CAVI)\relax }}{85}{algocf.6}}
\newlabel{alg:6}{{6}{85}{Coordinate Ascent Variational Inference Algorithm}{algocf.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Algorithms for ``Sprinkler" Experiment}{86}{appendix.B.3}}
\newlabel{app:B.3}{{B.3}{86}{Algorithms for ``Sprinkler" Experiment}{appendix.B.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Sprinkler Prior-Contrastive Algorithm\relax }}{86}{algocf.7}}
\newlabel{alg:7}{{7}{86}{Algorithms for ``Sprinkler" Experiment}{algocf.7}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Sprinkler Joint-Contrastive Algorithm\relax }}{87}{algocf.8}}
\newlabel{alg:8}{{8}{87}{Algorithms for ``Sprinkler" Experiment}{algocf.8}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix C}Coordinate Ascent Variational Inference Derivation}{88}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:mfvi}{{C}{88}{Coordinate Ascent Variational Inference Derivation}{appendix.C}{}}
\newlabel{mfvi2}{{C.0.1}{88}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.1}{}}
\newlabel{mfvi3}{{C.0.2}{88}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.2}{}}
\citation{pattern}
\newlabel{mfvi4}{{C.0.3}{89}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.3}{}}
\newlabel{mfvi5}{{C.0.4}{89}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.4}{}}
\newlabel{mfvi6}{{C.0.5}{89}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.5}{}}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix D}Mean Field Variational Inference Example}{90}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:C}{{D}{90}{Mean Field Variational Inference Example}{appendix.D}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces CAVI Algorithm for Bayesian mixture of Gaussians\relax }}{93}{algocf.9}}
\newlabel{alg:9}{{9}{93}{Mean Field Variational Inference Example}{algocf.9}{}}
\citation{kde}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix E}Kernel Density Estimation}{94}{appendix.E}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:D}{{E}{94}{Kernel Density Estimation}{appendix.E}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix F}Prior-Contrastive Optimal Estimator Experiment Plots}{95}{appendix.F}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:E}{{F}{95}{Prior-Contrastive Optimal Estimator Experiment Plots}{appendix.F}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.1}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{95}{figure.caption.59}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.2}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{95}{figure.caption.60}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.3}{\ignorespaces Prior-Contrastive $NELBO$\relax }}{95}{figure.caption.61}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix G}Second Functional Derivatives of Direct Log Ratio Estimator Losses}{96}{appendix.G}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:F}{{G}{96}{Second Functional Derivatives of Direct Log Ratio Estimator Losses}{appendix.G}{}}
