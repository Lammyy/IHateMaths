\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{s-intro}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Context}{1}}
\citation{DeepLearning}
\citation{neuroplast}
\citation{neuroplast}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 2}Background on Neural Networks}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{2}}
\citation{DeepLearning}
\citation{universal,cybenko}
\citation{mnist}
\citation{neuralstat}
\citation{haykin}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Individual Node Structure}{3}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example structure of an individual node function with 3 inputs, labelled as $\bm  {x}=[x_0\hskip 1em\relax x_1\hskip 1em\relax x_2\hskip 1em\relax x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm  {\theta }=[\theta _0\hskip 1em\relax \theta _1\hskip 1em\relax \theta _2\hskip 1em\relax \theta _3]$.\relax }}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{4}}
\citation{snyman}
\citation{wu}
\citation{cybenko}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation Function Plots\relax }}{5}}
\citation{neuralstat}
\citation{DeepLearning}
\citation{sparse}
\citation{kolen}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Structure}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example Neural Network Structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in Section 2.2: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm  {\Theta }$ denotes the weights of the network.\relax }}{7}}
\citation{bishop}
\citation{xavier}
\citation{goodman}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Weight Initialisation}{9}}
\citation{DeepLearning}
\citation{optimneural}
\citation{optim}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimisation}{10}}
\citation{floudas}
\citation{batch,optimneural}
\citation{bengio}
\citation{optimneural}
\citation{adam}
\citation{backprop}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Back-Propagation}{12}}
\citation{DeepLearning}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Back-Propagation Algorithm\relax }}{14}}
\citation{gelman}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 3}Variational Inference}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Context}{15}}
\citation{KL}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The KL Divergence}{16}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Introduction to Variational Inference}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Derivation of the ELBO}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Mean-Field Variational Family}{20}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Coordinate Ascent Variational Inference (CAVI)\relax }}{22}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Example: Bayesian Mixture of Gaussians}{23}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces CAVI Algorithm for Bayesian mixture of Gaussians\relax }}{26}}
\citation{ADVVI}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Amortized Inference}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces DAG for Mean-Field Variational Inference\relax }}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces DAG for Amortized Inference\relax }}{27}}
\citation{kingma}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Example: Variational Autoencoder}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Variational Autoencoder\relax }}{30}}
\citation{mescheder}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Problems with Implicit Distributions}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Implicit Prior and/or Variational Posterior}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Adversarial Variational Bayes\relax }}{31}}
\citation{ali}
\citation{tran}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Implicit Likelihood}{32}}
\citation{tiao}
\citation{sugiyama,mohamed}
\citation{huszar}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 4}Density Ratio Estimation}{34}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Class Probability Estimation}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Derivation}{34}}
\citation{sugiyama}
\citation{gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Prior-Contrastive Algorithm}{36}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Prior-Contrastive Class Probability Estimation\relax }}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Joint-Contrastive Algorithm}{39}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Joint-Contrastive Class Probability Estimation\relax }}{40}}
\citation{nguyen}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divergence Minimisation}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Prior-Contrastive Algorithm}{43}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Prior-Contrastive Divergence Minimisation\relax }}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Joint-Contrastive Algorithm}{45}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Joint-Contrastive Divergence Minimisation\relax }}{46}}
\citation{tiao}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Alternative Derivation of Class Probability Estimation}{47}}
\citation{huszar}
\citation{explain}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 5}Initial Experiment - Inference, ``Sprinkler" Example}{49}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Problem Context}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces True Posterior Plots for Sprinkler Experiment\relax }}{50}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Program Structure}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Generator Network Structure for Sprinkler Experiment\relax }}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Estimator Network Structure for Sprinkler Experiment\relax }}{51}}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Sprinkler Prior-Contrastive Class Probability Estimation\relax }}{53}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces Sprinkler Prior-Contrastive KL Divergence Minimisation\relax }}{54}}
\@writefile{loa}{\contentsline {algocf}{\numberline {10}{\ignorespaces Sprinkler Joint-Contrastive Class Probability Estimation\relax }}{55}}
\@writefile{loa}{\contentsline {algocf}{\numberline {11}{\ignorespaces Sprinkler Joint-Contrastive KL Divergence Minimisation\relax }}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{57}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \relax }}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Example Posterior Outputs\relax }}{57}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Average KL Divergences (KDE Estimate)\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Estimator Losses\relax }}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces NELBOs\relax }}{58}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 6}Theory Crafting}{60}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Recap because its confusing}{61}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}First and Second Derivatives of all 6 possible algorithms}{62}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Difference between divergences}{64}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Difference between estimators}{64}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 7}Inference Experiments - "Sprinkler"}{68}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Determining optimal activation function}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Experiment Outline}{68}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Results}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Comparing Estimator Accuracies}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Experiment Outline}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Results}{69}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Comparing Accuracies of Undertrained Estimators}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Experiment Outline}{70}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Results}{70}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 8}Data Generation - (MNIST image generation)}{71}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Problem Context}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Program Structure}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Results}{72}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 9}Related Work and Discussion}{74}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 10}Conclusion}{75}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{apalike}
\bibdata{bible}
\bibcite{bishop}{Bishop, 1995}
\bibcite{blei}{Blei et\nobreakspace  {}al., 2017}
\bibcite{optim}{Boyd and Vandenberghe, 2004}
\bibcite{neuralstat}{Cheng and Titterington, 1994}
\bibcite{cybenko}{Cybenko, 1989}
\bibcite{ali}{{Dumoulin} et\nobreakspace  {}al., 2016}
\bibcite{backprop}{E.\nobreakspace  {}Rumelhart et\nobreakspace  {}al., 1986}
\bibcite{floudas}{Floudas, 2005}
\bibcite{gelman}{Gelman et\nobreakspace  {}al., 2004}
\bibcite{xavier}{Glorot and Bengio, 2010}
\bibcite{sparse}{Glorot et\nobreakspace  {}al., 2011}
\@writefile{toc}{\contentsline {chapter}{References}{76}}
\bibcite{DeepLearning}{Goodfellow et\nobreakspace  {}al., 2016}
\bibcite{gan}{Goodfellow et\nobreakspace  {}al., 2014}
\bibcite{goodman}{Goodman, 1960}
\bibcite{haykin}{Haykin, 1998}
\bibcite{universal}{Hornik, 1991}
\bibcite{huszar}{{Husz{\'a}r}, 2017}
\bibcite{adam}{Kingma and Ba, 2014}
\bibcite{kingma}{{Kingma} and {Welling}, 2013}
\bibcite{kolen}{Kolen and Kremer, 2001}
\bibcite{KL}{Kullback, 1959}
\bibcite{mescheder}{Mescheder et\nobreakspace  {}al., 2017}
\bibcite{mohamed}{{Mohamed} and {Lakshminarayanan}, 2016}
\bibcite{nguyen}{Nguyen et\nobreakspace  {}al., 2010}
\bibcite{explain}{P.\nobreakspace  {}Wellman and Henrion, 1993}
\bibcite{optimneural}{Ruder, 2016}
\bibcite{mnist}{Simard et\nobreakspace  {}al., 2003}
\bibcite{snyman}{Snyman, 2005}
\bibcite{sugiyama}{Sugiyama et\nobreakspace  {}al., 2012}
\bibcite{tiao}{Tiao et\nobreakspace  {}al., 2018}
\bibcite{tran}{{Tran} et\nobreakspace  {}al., 2017}
\bibcite{wu}{Wu, 2009}
\bibcite{ADVVI}{Zhang et\nobreakspace  {}al., 2017}
\bibcite{neuroplast}{Zilles, 1992}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix A}Kernel Density Estimation}{79}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
