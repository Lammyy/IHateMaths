\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{s-intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Context}{1}{section.1.1}}
\citation{DeepLearning}
\citation{neuroplast}
\citation{neuroplast}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 2}Background on Neural Networks}{2}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{2}{section.2.1}}
\citation{DeepLearning}
\citation{universal,cybenko}
\citation{mnist}
\citation{neuralstat}
\citation{haykin}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Individual Node Structure}{3}{section.2.2}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example structure of an individual node function with 3 inputs, labelled as $\bm  {x}=[x_0\hskip 1em\relax x_1\hskip 1em\relax x_2\hskip 1em\relax x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm  {\theta }=[\theta _0\hskip 1em\relax \theta _1\hskip 1em\relax \theta _2\hskip 1em\relax \theta _3]$.\relax }}{4}{figure.caption.6}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{4}{section.2.3}}
\citation{snyman}
\citation{wu}
\citation{cybenko}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation Function Plots\relax }}{5}{figure.caption.7}}
\citation{neuralstat}
\citation{DeepLearning}
\citation{sparse}
\citation{kolen}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Structure}{6}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example Neural Network Structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in Section 2.2: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm  {\Theta }$ denotes the weights of the network.\relax }}{7}{figure.caption.8}}
\citation{bishop}
\citation{xavier}
\citation{goodman}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Weight Initialisation}{9}{section.2.5}}
\citation{DeepLearning}
\citation{optimneural}
\citation{optim}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimisation}{10}{section.2.6}}
\citation{floudas}
\citation{batch,optimneural}
\citation{bengio}
\citation{optimneural}
\citation{adam}
\citation{backprop}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Back-Propagation}{12}{section.2.7}}
\citation{DeepLearning}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Back-Propagation Algorithm\relax }}{14}{algocf.1}}
\citation{gelman}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 3}Variational Inference}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Context}{15}{section.3.1}}
\citation{blei}
\citation{KL}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The KL Divergence}{16}{section.3.2}}
\newlabel{lemma:3.2.5}{{3.2.5}{17}{}{theorem.3.2.5}{}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Introduction to Variational Inference}{18}{section.3.3}}
\newlabel{eqn:3.3.1}{{3.3.1}{18}{Introduction to Variational Inference}{equation.3.3.1}{}}
\newlabel{3.3.2}{{3.3.2}{18}{Introduction to Variational Inference}{equation.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Derivation of the ELBO}{18}{section.3.4}}
\citation{pattern}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Mean-Field Variational Family}{20}{section.3.5}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Coordinate Ascent Variational Inference (CAVI)\relax }}{22}{algocf.2}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Example: Bayesian Mixture of Gaussians}{23}{section.3.6}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces CAVI Algorithm for Bayesian mixture of Gaussians\relax }}{26}{algocf.3}}
\citation{ADVVI}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Amortized Inference}{27}{section.3.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This is a DAG representing Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm  {\phi ^{(i)}}$ corresponding to each latent variable point $\bm  {z}^{(i)}$ has to be found. $\bm  {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }}{27}{figure.caption.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm  {\phi }$, and each data point $\bm  {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm  {\phi }}(\bm  {z}|\bm  {x})$ to find $\bm  {z}^{(i)}$.\relax }}{27}{figure.caption.12}}
\citation{kingma}
\citation{vae}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Example: Variational Autoencoder}{29}{section.3.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm  {x}$ is passed through the encoder $q_\phi (\bm  {z}|\bm  {x})$ to produce a mean vector $\bm  {\mu }$ and a variance vector $\bm  {\sigma }^2$. Random noise $\bm  {\epsilon }_1$ is sampled from $\mathcal  {N}(0,I_{M\times M})$ and transformed to generate $\bm  {z}$: $\bm  {z}=\bm  {\mu }+\bm  {\epsilon }\cdot \bm  {\sigma }^2$. This latent variable $\bm  {z}$ is passed through the decoder $p_\theta (\bm  {x}|\bm  {z})$ to reconstruct the data point as $\mathaccentV {tilde}07E{\bm  {x}}$.\relax }}{30}{figure.caption.13}}
\citation{mescheder}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip To generate new data $\mathaccentV {tilde}07E{\bm  {x}}$ similar to existing data $\bm  {x}$, we sample latent variable $\bm  {z}$ from the prior distribution $p(\bm  {z})$ and pass it through the decoder $p_\theta (\bm  {x}|\bm  {z})$.\relax }}{31}{figure.caption.14}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Problems with Implicit Distributions}{31}{section.3.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Implicit Prior and/or Variational Posterior}{31}{subsection.3.9.1}}
\citation{ali}
\citation{tran}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\epsilon _1$ according to the mean vector $\bm  {\mu }$ and variance vector $\bm  {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal  {G}_\phi (\epsilon _1;\bm  {x})$ directly as an additional input. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ has the same explicit representation as in Figures 3.3 and 3.4.\relax }}{32}{figure.caption.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.2}Implicit Likelihood}{32}{subsection.3.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts a variation of ``Adversarial Variational Bayes", in which noise is additionally added to the input of the generator of likelihood distribution samples $G_\theta (\epsilon _2;\bm  {x})$. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ is therefore implicit and consequently, its corresponding term in the optimisation problem cannot be evaluated.\relax }}{33}{figure.caption.16}}
\citation{tiao}
\citation{sugiyama,mohamed}
\citation{huszar}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 4}Density Ratio Estimation}{35}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Class Probability Estimation}{35}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Derivation}{35}{subsection.4.1.1}}
\citation{sugiyama}
\citation{gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Prior-Contrastive Algorithm}{37}{subsection.4.1.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Prior-Contrastive Class Probability Estimation\relax }}{39}{algocf.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Joint-Contrastive Algorithm}{40}{subsection.4.1.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Joint-Contrastive Class Probability Estimation\relax }}{41}{algocf.5}}
\citation{nguyen}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divergence Minimisation}{42}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation}{42}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Prior-Contrastive Algorithm}{44}{subsection.4.2.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Prior-Contrastive Divergence Minimisation\relax }}{45}{algocf.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Joint-Contrastive Algorithm}{46}{subsection.4.2.3}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Joint-Contrastive Divergence Minimisation\relax }}{47}{algocf.7}}
\citation{tiao}
\citation{JS}
\citation{gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Alternative Derivation of Class Probability Estimation}{48}{subsection.4.2.4}}
\citation{huszar}
\citation{explain}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 5}Initial Experiment - Inference, ``Sprinkler" Example}{50}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Problem Context}{50}{section.5.1}}
\citation{gan}
\citation{relu1,relu2}
\citation{gan}
\citation{relu1,relu2}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly multimodal and unusually shaped. Clearly a flexible model for the posterior distribution is required, as a typical multivariate Gaussian model would fail to capture these features.\relax }}{51}{figure.caption.21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Program Structure}{51}{section.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\epsilon })$ used in the ``Continuous Sprinkler" experiment. The number inside the node indicates how many nodes the layer has, and the text above the node describes the activation function. Recall that the input layer does not have an activation function, and Rectified Linear Units (ReLU) are used for most of the hidden layers due to their many advantages. In the Concat. layer, the two input vectors are concatenated and there is no activation function. We do not use an activation function for the output layer as $q_\phi (z|x)\in \mathbb  {R}$.\relax }}{52}{figure.caption.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in Figure 5.2. When the estimator takes the parametrisation of a discriminator $D_\alpha (z,x)\simeq \frac  {q(z,x)}{q(z,x)+p(z,x)}$, a sigmoid activation function is used for the output layer \citep  {gan}. A ReLU output layer is used when the estimator output is the direct density ratio $r_\alpha (z,x)\simeq \frac  {q(z,x)}{p(z,x)}$ \citep  {relu1, relu2}.\relax }}{52}{figure.caption.23}}
\citation{DeepLearning}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Sprinkler Prior-Contrastive Algorithm\relax }}{54}{algocf.8}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces Sprinkler Joint-Contrastive Algorithm\relax }}{55}{algocf.9}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{56}{section.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces \relax }}{56}{table.caption.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Initial tests show that the variational posterior reaches optimality at an average KL divergence of about 1.325: as seen in sub-figure (a), this corresponds to outputs similar to the true posterior in sub-figure (c). The posterior output in sub-figure (b) with a larger average KL divergence of 1.3963 is less flexible and only uni-modal for $x=50$.\relax }}{56}{figure.caption.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Average KL Divergences (KDE Estimate)\relax }}{57}{figure.caption.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Estimator Losses\relax }}{57}{figure.caption.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces NELBOs\relax }}{57}{figure.caption.30}}
\citation{sparse}
\citation{leaky}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 6}Activation Function Experiment}{59}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Theory}{59}{section.6.1}}
\citation{nowozin}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Experiment Outline}{61}{section.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Results}{61}{section.6.3}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \relax }}{61}{table.caption.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Average KL Divergences (KDE Estimate)\relax }}{62}{figure.caption.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Estimator Losses\relax }}{62}{figure.caption.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces NELBOs\relax }}{62}{figure.caption.34}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 7}Algorithm Analysis}{64}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{64}{section.7.1}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Algorithm Generalisation}{65}{section.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Reverse KL Divergence}{65}{subsection.7.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}GAN Divergence}{65}{subsection.7.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Optimization Algorithms}{66}{section.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Prior-Contrastive}{66}{subsection.7.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Joint-Contrastive}{67}{subsection.7.3.2}}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 8}Comparing Optimal Estimators}{68}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Theory}{68}{section.8.1}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Experiment Outline}{68}{section.8.2}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Results}{69}{section.8.3}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Prior-Contrastive Results\relax }}{69}{table.caption.35}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces Joint-Contrastive Results\relax }}{69}{table.caption.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{71}{figure.caption.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{71}{figure.caption.38}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Prior-Contrastive NELBO\relax }}{71}{figure.caption.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Joint-Contrastive Average KL Divergence\relax }}{72}{figure.caption.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces Joint-Contrastive Estimator Loss\relax }}{72}{figure.caption.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Joint-Contrastive NELBO\relax }}{72}{figure.caption.42}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 9}Comparing Undertrained Estimators}{73}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Theory}{73}{section.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Estimator Bounds}{73}{subsection.9.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Displacement of Estimator Optimal Values}{74}{subsection.9.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we have plotted the estimator output against its loss functional. The same x and y-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the f-divergences.\relax }}{75}{figure.caption.43}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Experiment Outline}{77}{section.9.2}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Results}{77}{section.9.3}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Prior-Contrastive Results\relax }}{78}{table.caption.44}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Joint-Contrastive Results\relax }}{78}{table.caption.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{79}{figure.caption.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{79}{figure.caption.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Prior-Contrastive NELBO\relax }}{79}{figure.caption.48}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Joint-Contrastive Average KL Divergence\relax }}{80}{figure.caption.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Joint-Contrastive Estimator Loss\relax }}{80}{figure.caption.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Joint-Contrastive NELBO\relax }}{80}{figure.caption.51}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 10}Data Generation - (MNIST image generation)}{81}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Problem Context}{81}{section.10.1}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Program Structure}{81}{section.10.2}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Results}{82}{section.10.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 11}Related Work and Discussion}{84}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 12}Conclusion}{85}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibstyle{apalike}
\bibdata{bible}
\bibcite{bengio}{{1}{2012}{{Bengio}}{{}}}
\bibcite{bishop}{{2}{1995}{{Bishop}}{{}}}
\bibcite{pattern}{{3}{2006}{{Bishop}}{{}}}
\bibcite{blei}{{4}{2017}{{Blei et~al.}}{{}}}
\bibcite{optim}{{5}{2004}{{Boyd and Vandenberghe}}{{}}}
\bibcite{neuralstat}{{6}{1994}{{Cheng and Titterington}}{{}}}
\bibcite{cybenko}{{7}{1989}{{Cybenko}}{{}}}
\bibcite{vae}{{8}{2016}{{Doersch}}{{}}}
\bibcite{ali}{{9}{2016}{{{Dumoulin} et~al.}}{{}}}
\bibcite{backprop}{{10}{1986}{{E.~Rumelhart et~al.}}{{}}}
\bibcite{floudas}{{11}{2005}{{Floudas}}{{}}}
\bibcite{JS}{{12}{2004}{{Fuglede and Topsoe}}{{}}}
\bibcite{gelman}{{13}{2004}{{Gelman et~al.}}{{}}}
\bibcite{xavier}{{14}{2010}{{Glorot and Bengio}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{86}{chapter.12}}
\bibcite{sparse}{{15}{2011}{{Glorot et~al.}}{{}}}
\bibcite{DeepLearning}{{16}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{gan}{{17}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{goodman}{{18}{1960}{{Goodman}}{{}}}
\bibcite{haykin}{{19}{1998}{{Haykin}}{{}}}
\bibcite{universal}{{20}{1991}{{Hornik}}{{}}}
\bibcite{huszar}{{21}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{adam}{{22}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kingma}{{23}{2013}{{{Kingma} and {Welling}}}{{}}}
\bibcite{kolen}{{24}{2001}{{Kolen and Kremer}}{{}}}
\bibcite{KL}{{25}{1959}{{Kullback}}{{}}}
\bibcite{batch}{{26}{2014}{{Li et~al.}}{{}}}
\bibcite{relu2}{{27}{2017}{{Liu et~al.}}{{}}}
\bibcite{leaky}{{28}{2013}{{Maas}}{{}}}
\bibcite{mescheder}{{29}{2017}{{Mescheder et~al.}}{{}}}
\bibcite{mohamed}{{30}{2016}{{{Mohamed} and {Lakshminarayanan}}}{{}}}
\bibcite{relu1}{{31}{2015}{{Nam and Sugiyama}}{{}}}
\bibcite{nguyen}{{32}{2010}{{Nguyen et~al.}}{{}}}
\bibcite{nowozin}{{33}{2016}{{Nowozin et~al.}}{{}}}
\bibcite{explain}{{34}{1993}{{P.~Wellman and Henrion}}{{}}}
\bibcite{optimneural}{{35}{2016}{{Ruder}}{{}}}
\bibcite{mnist}{{36}{2003}{{Simard et~al.}}{{}}}
\bibcite{snyman}{{37}{2005}{{Snyman}}{{}}}
\bibcite{sugiyama}{{38}{2012}{{Sugiyama et~al.}}{{}}}
\bibcite{tiao}{{39}{2018}{{Tiao et~al.}}{{}}}
\bibcite{tran}{{40}{2017}{{{Tran} et~al.}}{{}}}
\bibcite{wu}{{41}{2009}{{Wu}}{{}}}
\bibcite{ADVVI}{{42}{2017}{{Zhang et~al.}}{{}}}
\bibcite{neuroplast}{{43}{1992}{{Zilles}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix A}Kernel Density Estimation}{89}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
