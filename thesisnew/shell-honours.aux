\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{gelman}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Aims}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Context}{1}{section.1.2}}
\citation{ADVVI}
\citation{kingma}
\citation{mescheder}
\citation{sugiyama}
\citation{huszar}
\citation{kingma}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{3}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Structure}{4}{section.1.4}}
\citation{DeepLearning}
\citation{neuroplast}
\citation{neuroplast}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 2}Background on Neural Networks}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch2}{{2}{5}{Background on Neural Networks}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{5}{section.2.1}}
\newlabel{sec:2.1}{{2.1}{5}{Motivation}{section.2.1}{}}
\citation{DeepLearning}
\citation{universal,cybenko}
\citation{mnist}
\citation{neuralstat}
\citation{haykin}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Individual Node Structure}{6}{section.2.2}}
\newlabel{sec:2.2}{{2.2}{6}{Individual Node Structure}{section.2.2}{}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example structure of an individual node function with 3 inputs, labelled as $\bm  {x}=[x_0\hskip 1em\relax x_1\hskip 1em\relax x_2\hskip 1em\relax x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm  {\theta }=[\theta _0\hskip 1em\relax \theta _1\hskip 1em\relax \theta _2\hskip 1em\relax \theta _3]$.\relax }}{7}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2.1}{{2.1}{7}{\small Example structure of an individual node function with 3 inputs, labelled as $\bm {x}=[x_0\quad x_1\quad x_2\quad x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm {\theta }=[\theta _0\quad \theta _1\quad \theta _2\quad \theta _3]$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{7}{section.2.3}}
\newlabel{sec:2.3}{{2.3}{7}{Activation Functions}{section.2.3}{}}
\citation{snyman}
\citation{wu}
\citation{cybenko}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation Function Plots\relax }}{8}{figure.caption.7}}
\newlabel{fig:2.2}{{2.2}{8}{Activation Function Plots\relax }{figure.caption.7}{}}
\citation{neuralstat}
\citation{DeepLearning}
\citation{sparse}
\citation{kolen}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Structure}{9}{section.2.4}}
\newlabel{sec:2.4}{{2.4}{9}{Neural Network Structure}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of a neural network structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in \autoref  {sec:2.2}: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm  {\Theta }$ denotes the weights of the network.\relax }}{10}{figure.caption.8}}
\newlabel{fig:2.3}{{2.3}{10}{\small Example of a neural network structure with 3 external inputs, 1 hidden layer with 3 nodes, 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in \autoref {sec:2.2}: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm {\Theta }$ denotes the weights of the network.\relax }{figure.caption.8}{}}
\citation{bishop}
\citation{xavier}
\citation{goodman}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Weight Initialisation}{11}{section.2.5}}
\newlabel{sec:2.5}{{2.5}{11}{Weight Initialisation}{section.2.5}{}}
\citation{DeepLearning}
\citation{optimneural}
\citation{optim}
\citation{beck}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimisation}{12}{section.2.6}}
\newlabel{sec:2.6}{{2.6}{12}{Optimisation}{section.2.6}{}}
\citation{floudas}
\citation{batch,optimneural}
\citation{bengio}
\citation{optimneural}
\citation{adam}
\citation{backprop}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Back-Propagation}{14}{section.2.7}}
\newlabel{sec:2.7}{{2.7}{14}{Back-Propagation}{section.2.7}{}}
\citation{gelman}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 3}Variational Inference}{16}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch3}{{3}{16}{Variational Inference}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Context}{16}{section.3.1}}
\newlabel{sec:3.1}{{3.1}{16}{Context}{section.3.1}{}}
\citation{blei}
\citation{KL}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The KL Divergence}{17}{section.3.2}}
\newlabel{sec:3.2}{{3.2}{17}{The KL Divergence}{section.3.2}{}}
\newlabel{def:3.2.1}{{3.2.1}{17}{}{theorem.3.2.1}{}}
\newlabel{3.2.5}{{3.2.5}{18}{}{theorem.3.2.5}{}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Introduction to Variational Inference}{19}{section.3.3}}
\newlabel{sec:3.3}{{3.3}{19}{Introduction to Variational Inference}{section.3.3}{}}
\newlabel{eqn:3.3.1}{{3.3.1}{19}{Introduction to Variational Inference}{equation.3.3.1}{}}
\newlabel{eqn:3.3.2}{{3.3.2}{19}{Introduction to Variational Inference}{equation.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Derivation of the ELBO}{19}{section.3.4}}
\newlabel{sec:3.4}{{3.4}{19}{Derivation of the ELBO}{section.3.4}{}}
\citation{pattern}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Mean-Field Variational Family}{21}{section.3.5}}
\newlabel{sec:3.5}{{3.5}{21}{Mean-Field Variational Family}{section.3.5}{}}
\citation{pattern}
\citation{blei}
\citation{ADVVI}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Amortized Inference}{23}{section.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm  {\phi ^{(i)}}$ corresponding to each latent variable point $\bm  {z}^{(i)}$ has to be found. $\bm  {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }}{24}{figure.caption.9}}
\newlabel{fig:3.1}{{3.1}{24}{\small This DAG represents Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm {\phi ^{(i)}}$ corresponding to each latent variable point $\bm {z}^{(i)}$ has to be found. $\bm {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm  {\phi }$, and each data point $\bm  {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm  {\phi }}(\bm  {z}|\bm  {x})$ to find $\bm  {z}^{(i)}$.\relax }}{24}{figure.caption.10}}
\newlabel{fig:3.2}{{3.2}{24}{\small This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm {\phi }$, and each data point $\bm {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm {\phi }}(\bm {z}|\bm {x})$ to find $\bm {z}^{(i)}$.\relax }{figure.caption.10}{}}
\citation{kingma}
\citation{vae}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Example: Variational Autoencoder}{25}{section.3.7}}
\newlabel{sec:3.7}{{3.7}{25}{Example: Variational Autoencoder}{section.3.7}{}}
\citation{mescheder}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm  {x}$ is passed through the encoder $q_\phi (\bm  {z}|\bm  {x})$ to produce a mean vector $\bm  {\mu }$ and a variance vector $\bm  {\sigma }^2$. Random noise $\bm  {\epsilon }_1$ is sampled from $\mathcal  {N}(0,I_{M\times M})$ and transformed to generate $\bm  {z}$: $\bm  {z}=\bm  {\mu }+\bm  {\epsilon }\cdot \bm  {\sigma }^2$. This latent variable $\bm  {z}$ is passed through the decoder $p_\theta (\bm  {x}|\bm  {z})$ to reconstruct the data point as $\mathaccentV {tilde}07E{\bm  {x}}$.\relax }}{27}{figure.caption.11}}
\newlabel{fig:3.3}{{3.3}{27}{\small A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm {x}$ is passed through the encoder $q_\phi (\bm {z}|\bm {x})$ to produce a mean vector $\bm {\mu }$ and a variance vector $\bm {\sigma }^2$. Random noise $\bm {\epsilon }_1$ is sampled from $\mathcal {N}(0,I_{M\times M})$ and transformed to generate $\bm {z}$: $\bm {z}=\bm {\mu }+\bm {\epsilon }\cdot \bm {\sigma }^2$. This latent variable $\bm {z}$ is passed through the decoder $p_\theta (\bm {x}|\bm {z})$ to reconstruct the data point as $\tilde {\bm {x}}$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip To generate new data $\mathaccentV {tilde}07E{\bm  {x}}$ similar to existing data $\bm  {x}$, we sample latent variable $\bm  {z}$ from the prior distribution $p(\bm  {z})$ and pass it through the decoder $p_\theta (\bm  {x}|\bm  {z})$.\relax }}{27}{figure.caption.12}}
\newlabel{fig:3.4}{{3.4}{27}{\small To generate new data $\tilde {\bm {x}}$ similar to existing data $\bm {x}$, we sample latent variable $\bm {z}$ from the prior distribution $p(\bm {z})$ and pass it through the decoder $p_\theta (\bm {x}|\bm {z})$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Problems with Implicit Distributions}{27}{section.3.8}}
\newlabel{sec:3.8}{{3.8}{27}{Problems with Implicit Distributions}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Implicit Prior and/or Variational Posterior}{27}{subsection.3.8.1}}
\newlabel{sec:3.8.1}{{3.8.1}{27}{Implicit Prior and/or Variational Posterior}{subsection.3.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\epsilon _1$ according to the mean vector $\bm  {\mu }$ and variance vector $\bm  {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal  {G}_\phi (\epsilon _1;\bm  {x})$ directly as an additional input. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ has the same explicit representation as in Figures \ref  {fig:3.3} and \ref  {fig:3.4}.\relax }}{28}{figure.caption.13}}
\newlabel{fig:3.5}{{3.5}{28}{\small This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\epsilon _1$ according to the mean vector $\bm {\mu }$ and variance vector $\bm {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal {G}_\phi (\epsilon _1;\bm {x})$ directly as an additional input. The likelihood distribution $p_\theta (\bm {x}|\bm {z})$ has the same explicit representation as in Figures \ref {fig:3.3} and \ref {fig:3.4}.\relax }{figure.caption.13}{}}
\citation{ali}
\citation{tran}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Implicit Likelihood}{29}{subsection.3.8.2}}
\newlabel{sec:3.8.2}{{3.8.2}{29}{Implicit Likelihood}{subsection.3.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts a variation of ``Adversarial Variational Bayes", in which noise is additionally added to the input of the generator of likelihood distribution samples $G_\theta (\epsilon _2;\bm  {x})$. The likelihood distribution $p_\theta (\bm  {x}|\bm  {z})$ is therefore implicit and consequently, its corresponding term in the optimisation problem cannot be evaluated.\relax }}{29}{figure.caption.14}}
\newlabel{fig:3.6}{{3.6}{29}{\small This diagram depicts a variation of ``Adversarial Variational Bayes", in which noise is additionally added to the input of the generator of likelihood distribution samples $G_\theta (\epsilon _2;\bm {x})$. The likelihood distribution $p_\theta (\bm {x}|\bm {z})$ is therefore implicit and consequently, its corresponding term in the optimisation problem cannot be evaluated.\relax }{figure.caption.14}{}}
\citation{tiao}
\citation{sugiyama,mohamed}
\citation{huszar}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 4}Density Ratio Estimation}{31}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch4}{{4}{31}{Density Ratio Estimation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Class Probability Estimation}{31}{section.4.1}}
\newlabel{sec:4.1}{{4.1}{31}{Class Probability Estimation}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Derivation}{31}{subsection.4.1.1}}
\newlabel{sec:4.1.1}{{4.1.1}{31}{Derivation}{subsection.4.1.1}{}}
\citation{sugiyama}
\citation{gan}
\newlabel{rem:4.1.2}{{4.1.2}{32}{}{theorem.4.1.2}{}}
\newlabel{rem:4.1.3}{{4.1.3}{32}{}{theorem.4.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Prior-Contrastive Algorithm}{33}{subsection.4.1.2}}
\newlabel{sec:4.1.2}{{4.1.2}{33}{Prior-Contrastive Algorithm}{subsection.4.1.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Prior-Contrastive Class Probability Estimation\relax }}{35}{algocf.1}}
\newlabel{alg:1}{{1}{35}{Prior-Contrastive Algorithm}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Joint-Contrastive Algorithm}{35}{subsection.4.1.3}}
\newlabel{sec:4.1.3}{{4.1.3}{35}{Joint-Contrastive Algorithm}{subsection.4.1.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Joint-Contrastive Class Probability Estimation\relax }}{37}{algocf.2}}
\newlabel{alg:2}{{2}{37}{Joint-Contrastive Algorithm}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divergence Minimisation}{37}{section.4.2}}
\newlabel{sec:4.2}{{4.2}{37}{Divergence Minimisation}{section.4.2}{}}
\citation{nguyen}
\citation{nguyen}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation}{38}{subsection.4.2.1}}
\newlabel{sec:4.2.1}{{4.2.1}{38}{Derivation}{subsection.4.2.1}{}}
\newlabel{4.2.1}{{4.2.1}{38}{}{theorem.4.2.1}{}}
\newlabel{rem:4.2.4}{{4.2.4}{39}{}{theorem.4.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Prior-Contrastive Algorithm}{39}{subsection.4.2.2}}
\newlabel{sec:4.2.2}{{4.2.2}{39}{Prior-Contrastive Algorithm}{subsection.4.2.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Prior-Contrastive Divergence Minimisation\relax }}{40}{algocf.3}}
\newlabel{alg:3}{{3}{40}{Prior-Contrastive Algorithm}{algocf.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Joint-Contrastive Algorithm}{41}{subsection.4.2.3}}
\newlabel{sec:4.2.3}{{4.2.3}{41}{Joint-Contrastive Algorithm}{subsection.4.2.3}{}}
\citation{tiao}
\citation{JS}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Joint-Contrastive Divergence Minimisation\relax }}{42}{algocf.4}}
\newlabel{alg:4}{{4}{42}{Joint-Contrastive Algorithm}{algocf.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Alternative Derivation of Class Probability Estimation}{42}{subsection.4.2.4}}
\newlabel{sec:4.2.4}{{4.2.4}{42}{Alternative Derivation of Class Probability Estimation}{subsection.4.2.4}{}}
\citation{gan}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 5}Algorithm Generalisation}{45}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch5}{{5}{45}{Algorithm Generalisation}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{45}{section.5.1}}
\newlabel{sec:5.1}{{5.1}{45}{Introduction}{section.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm Generalisation}{46}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Reverse KL Divergence}{46}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}GAN Divergence}{46}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Optimization Algorithms}{47}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Prior-Contrastive}{47}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Joint-Contrastive}{48}{subsection.5.3.2}}
\citation{nowozin}
\citation{huszar}
\citation{explain}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 6}Comparing Optimal Estimators}{49}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch6}{{6}{49}{Comparing Optimal Estimators}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Theory}{49}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Problem Context}{49}{section.6.2}}
\citation{gan}
\citation{nowozin}
\citation{gan}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly multimodal and unusually shaped. Clearly a flexible model for the posterior distribution is required, as a typical multivariate Gaussian model would fail to capture these features.\relax }}{51}{figure.caption.19}}
\newlabel{fig:6.1}{{6.1}{51}{\small This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly multimodal and unusually shaped. Clearly a flexible model for the posterior distribution is required, as a typical multivariate Gaussian model would fail to capture these features.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Program Structure}{51}{section.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\epsilon })$ used in the ``Continuous Sprinkler" experiment. The number inside the node indicates how many nodes the layer has, and the text above the node describes the activation function. Recall that the input layer does not have an activation function, and Rectified Linear Units (ReLU) are used for most of the hidden layers due to their many advantages. In the Concat. layer, the two input vectors are concatenated and there is no activation function. We do not use an activation function for the output layer as $q_\phi (z|x)\in \mathbb  {R}$.\relax }}{51}{figure.caption.20}}
\newlabel{fig:6.2}{{6.2}{51}{\small This figure illustrates the structure of the generator network $\mathcal {G}(x,\bm {\epsilon })$ used in the ``Continuous Sprinkler" experiment. The number inside the node indicates how many nodes the layer has, and the text above the node describes the activation function. Recall that the input layer does not have an activation function, and Rectified Linear Units (ReLU) are used for most of the hidden layers due to their many advantages. In the Concat. layer, the two input vectors are concatenated and there is no activation function. We do not use an activation function for the output layer as $q_\phi (z|x)\in \R $.\relax }{figure.caption.20}{}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in \autoref  {fig:6.2}. When the estimator takes the parametrisation of a discriminator $D_\alpha (z,x)$, a sigmoid output layer is used \citep  {gan}. An exponential activation is used for the output layer when the estimator output is the direct density ratio $r_\alpha (z,x)$, and a linear output is used for the direct log density ratio estimator $T_\alpha (z,x)$ \citep  {nowozin}.\relax }}{52}{figure.caption.21}}
\newlabel{fig:6.3}{{6.3}{52}{\small This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in \autoref {fig:6.2}. When the estimator takes the parametrisation of a discriminator $D_\alpha (z,x)$, a sigmoid output layer is used \citep {gan}. An exponential activation is used for the output layer when the estimator output is the direct density ratio $r_\alpha (z,x)$, and a linear output is used for the direct log density ratio estimator $T_\alpha (z,x)$ \citep {nowozin}.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Results}{53}{section.6.4}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This table presents the results from the prior-contrastive experiment. Since the programs reached optimality in this case, it is uncertain from those results whether the choice of $f$-divergence or estimator parametrisation impacts posterior convergence.\relax }}{54}{table.caption.22}}
\newlabel{tab:6.1}{{6.1}{54}{\small This table presents the results from the prior-contrastive experiment. Since the programs reached optimality in this case, it is uncertain from those results whether the choice of $f$-divergence or estimator parametrisation impacts posterior convergence.\relax }{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip From these joint-contrastive results, it is evident that for each $f$-divergence, similar posterior convergence is associated with the different estimator parametrisations. As Nowozin's paper suggests, the GAN divergence demonstrates slower and more inconsistent results than the reverse KL divergence.\relax }}{54}{table.caption.23}}
\newlabel{tab:6.2}{{6.2}{54}{\small From these joint-contrastive results, it is evident that for each $f$-divergence, similar posterior convergence is associated with the different estimator parametrisations. As Nowozin's paper suggests, the GAN divergence demonstrates slower and more inconsistent results than the reverse KL divergence.\relax }{table.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These plots of the true KL divergence in the joint-contrastive context show near identical convergence for all estimators.\relax }}{54}{figure.caption.24}}
\newlabel{fig:6.4}{{6.4}{54}{\small These plots of the true KL divergence in the joint-contrastive context show near identical convergence for all estimators.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In sub-figure (a), the estimators have similar levels of stability, but the class probability estimator loss appears to be higher than the other two estimator parametrisations. All three estimators in sub-figure (b) have similar, increasingly unstable losses.\relax }}{55}{figure.caption.25}}
\newlabel{fig:6.5}{{6.5}{55}{\small In sub-figure (a), the estimators have similar levels of stability, but the class probability estimator loss appears to be higher than the other two estimator parametrisations. All three estimators in sub-figure (b) have similar, increasingly unstable losses.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These joint-contrastive $NELBO$ plots generally mirror the estimator loss trends depicted in Figure 6.8. Lower and more stable $NELBO$ estimations are associated with the reverse KL divergence, correlating with its superior posterior convergence. It is unclear why the $NELBO$ increases over the iterations when its minimisation theoretically leads to posterior convergence.\relax }}{55}{figure.caption.26}}
\newlabel{fig:6.6}{{6.6}{55}{\small These joint-contrastive $NELBO$ plots generally mirror the estimator loss trends depicted in Figure 6.8. Lower and more stable $NELBO$ estimations are associated with the reverse KL divergence, correlating with its superior posterior convergence. It is unclear why the $NELBO$ increases over the iterations when its minimisation theoretically leads to posterior convergence.\relax }{figure.caption.26}{}}
\citation{lecun}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 7}Comparing Undertrained Estimators}{56}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch7}{{7}{56}{Comparing Undertrained Estimators}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Theory}{56}{section.7.1}}
\newlabel{sec:7.1}{{7.1}{56}{Theory}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Estimator Bounds}{56}{subsection.7.1.1}}
\citation{lecun}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}First and Second Derivatives of Estimator Loss Functions}{57}{subsection.7.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we have plotted the estimator output against its loss functional. The same x and y-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }}{58}{figure.caption.27}}
\newlabel{fig:7.1}{{7.1}{58}{\small Above we have plotted the estimator output against its loss functional. The same x and y-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }{figure.caption.27}{}}
\citation{nowozin}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Displacement of Estimator Optimal Values}{60}{subsection.7.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Experiment Outline}{61}{section.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{62}{section.7.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Prior-Contrastive Results\relax }}{62}{table.caption.30}}
\newlabel{tab:7.1}{{7.1}{62}{Prior-Contrastive Results\relax }{table.caption.30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Joint-Contrastive Results\relax }}{63}{table.caption.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{64}{figure.caption.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{64}{figure.caption.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Prior-Contrastive NELBO\relax }}{64}{figure.caption.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Joint-Contrastive Average KL Divergence\relax }}{65}{figure.caption.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Joint-Contrastive Estimator Loss\relax }}{65}{figure.caption.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Joint-Contrastive $NELBO$\relax }}{65}{figure.caption.37}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 8}Data Generation - (MNIST image generation)}{66}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch8}{{8}{66}{Data Generation - (MNIST image generation)}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Experiment Outline}{66}{section.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Samples from the MNIST Dataset\relax }}{66}{figure.caption.38}}
\newlabel{fig:8.1}{{8.1}{66}{Samples from the MNIST Dataset\relax }{figure.caption.38}{}}
\citation{nowozin,bgan}
\citation{nowozin,bgan}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\varepsilon })$ used in the MNIST data generation experiment. The notation used in the figure is the same as in Figures 5.2 and 5.3, and overall the network structure is very similar except significantly more nodes are used, suiting the higher dimensionality of the data. The dimensionality of latent variable $z$ is either 2 or 10 depending on the experimental setting. We have arbitrarily chosen the number of random noise inputs $\varepsilon $ to be 4.\relax }}{67}{figure.caption.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This is a diagram of the decoder network used in the MNIST data generation experiment. A sigmoid output layer is used to map the network output to $(0,1)$, suiting the grayscale nature of the data. The amount of effective hidden layers is the same as in the encoder network, but there are significantly more nodes in each layer. This is because the objective of the program is to output relatively high dimensional image data. Although this problem involves image analysis, due to the relatively small image size we refrain from using convolutional layers. This is consistent with other similar MNIST experiments \citep  {nowozin, bgan}.\relax }}{67}{figure.caption.40}}
\citation{nowozin,bgan,tiao}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the estimator network for the MNIST data generation experiment. Depending on the estimator parametrisation, the output activation function is either sigmoid for the class probability estimator $D_\alpha (z,x)\simeq \frac  {q_\phi (z|x)}{q_\phi (z|x)+p(z)}$, exponential for the direct ratio estimator $r_\alpha (z,x)\simeq \frac  {q_\phi (z|x)}{p(z)}$ or linear for the direct log ratio estimator $T_\alpha (z,x)\simeq \qopname  \relax o{log}\frac  {q_\phi (z|x)}{p(z)}$.\relax }}{68}{figure.caption.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder used in the MNIST data generation experiment. Note that there is no random noise added to the decoder.\relax }}{69}{figure.caption.42}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Low Dimensional Experiment Results}{69}{section.8.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Low Dimensional MNIST Autoencoder Experiment Results\relax }}{69}{table.caption.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip As shown in the above average reconstruction error plots, the direct log ratio estimator in sub-figure (a) consistently has a higher reconstruction error.\relax }}{70}{figure.caption.44}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Noting the scale of the estimator loss plot in sub-figure (b), it is evident that the reverse KL divergence leads to unstable estimator training. Unlike the estimators plotted in Figure 9.3 (b), these estimators don't appear to stabilise after a certain period. Sub-figure (a) shows that the direct log ratio estimator loss is consistently higher than the other two estimators, correlating with its poorer reconstruction.\relax }}{70}{figure.caption.45}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}High Dimensional Experiment Results}{70}{section.8.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Despite the apparent instability of the estimators trained with the reverse KL divergence, the $NELBO$ plot in sub-figure (b) is relatively consistent, leading to a smooth convergence as plotted in Figure 10.6 (b). The high estimator losses shown in Figure 10.7 (a) correspond to an overestimated $NELBO$ as depicted in sub-figure (a).\relax }}{71}{figure.caption.46}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces High Dimensional MNIST Autoencoder Experiment Results\relax }}{72}{table.caption.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This plot shows that the reconstruction error associated with the reverse KL divergence is consistently higher for the majority of the program runtime.\relax }}{72}{figure.caption.48}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The estimator loss corresponding to the reverse KL divergence appears to be extremely unstable for the entire program runtime, spiking to values exceeding $10^{14}$. On the other hand, the GAN divergence estimator loss appears to be relatively stable.\relax }}{73}{figure.caption.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The instability of the reverse KL divergence estimator loss propagates to its $NELBO$ estimation, which appears to fluctuate wildly in comparison to the GAN divergence's loss. This correlates to the superiority of the GAN divergence in this particular experiment. Again recalling Figures 9.3 and 9.6 (b), estimators formulated by the reverse KL divergence experience initial instability, but have more accurate density ratio estimation when stable. It is possible that the estimator in this experiment has not stabilised, leading to poorer network convergence. This may be due to the increased complexity of the distributions associated with the MNIST dataset, as opposed to the simplicity of the previous ``Sprinkler" problem.\relax }}{73}{figure.caption.50}}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 9}Conclusion and Further Research}{74}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch9}{{9}{74}{Conclusion and Further Research}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Further Research}{74}{section.9.1}}
\citation{bgan}
\citation{ali}
\citation{wang}
\citation{vincent}
\citation{ais}
\bibstyle{apalike}
\bibdata{bible}
\bibcite{beck}{{1}{2014}{{Beck}}{{}}}
\bibcite{bengio}{{2}{2012}{{Bengio}}{{}}}
\bibcite{bishop}{{3}{1995}{{Bishop}}{{}}}
\bibcite{pattern}{{4}{2006}{{Bishop}}{{}}}
\bibcite{blei}{{5}{2017}{{Blei et~al.}}{{}}}
\bibcite{optim}{{6}{2004}{{Boyd and Vandenberghe}}{{}}}
\bibcite{neuralstat}{{7}{1994}{{Cheng and Titterington}}{{}}}
\bibcite{cybenko}{{8}{1989}{{Cybenko}}{{}}}
\bibcite{vae}{{9}{2016}{{Doersch}}{{}}}
\bibcite{ali}{{10}{2016}{{{Dumoulin} et~al.}}{{}}}
\bibcite{backprop}{{11}{1986}{{E.~Rumelhart et~al.}}{{}}}
\bibcite{floudas}{{12}{2005}{{Floudas}}{{}}}
\bibcite{JS}{{13}{2004}{{Fuglede and Topsoe}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{76}{section.9.1}}
\bibcite{gelman}{{14}{2004}{{Gelman et~al.}}{{}}}
\bibcite{xavier}{{15}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{sparse}{{16}{2011}{{Glorot et~al.}}{{}}}
\bibcite{DeepLearning}{{17}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{gan}{{18}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{goodman}{{19}{1960}{{Goodman}}{{}}}
\bibcite{haykin}{{20}{1998}{{Haykin}}{{}}}
\bibcite{universal}{{21}{1991}{{Hornik}}{{}}}
\bibcite{huszar}{{22}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{adam}{{23}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kingma}{{24}{2013}{{{Kingma} and {Welling}}}{{}}}
\bibcite{kolen}{{25}{2001}{{Kolen and Kremer}}{{}}}
\bibcite{KL}{{26}{1959}{{Kullback}}{{}}}
\bibcite{lecun}{{27}{2012}{{LeCun et~al.}}{{}}}
\bibcite{batch}{{28}{2014}{{Li et~al.}}{{}}}
\bibcite{mescheder}{{29}{2017}{{Mescheder et~al.}}{{}}}
\bibcite{mohamed}{{30}{2016}{{{Mohamed} and {Lakshminarayanan}}}{{}}}
\bibcite{nguyen}{{31}{2010}{{Nguyen et~al.}}{{}}}
\bibcite{nowozin}{{32}{2016}{{Nowozin et~al.}}{{}}}
\bibcite{explain}{{33}{1993}{{P.~Wellman and Henrion}}{{}}}
\bibcite{optimneural}{{34}{2016}{{Ruder}}{{}}}
\bibcite{mnist}{{35}{2003}{{Simard et~al.}}{{}}}
\bibcite{snyman}{{36}{2005}{{Snyman}}{{}}}
\bibcite{sugiyama}{{37}{2012}{{Sugiyama et~al.}}{{}}}
\bibcite{tiao}{{38}{2018}{{Tiao et~al.}}{{}}}
\bibcite{tran}{{39}{2017}{{{Tran} et~al.}}{{}}}
\bibcite{bgan}{{40}{2016}{{{Uehara} et~al.}}{{}}}
\bibcite{vincent}{{41}{2008}{{Vincent et~al.}}{{}}}
\bibcite{wang}{{42}{2009}{{Wang et~al.}}{{}}}
\bibcite{wu}{{43}{2009}{{Wu}}{{}}}
\bibcite{ais}{{44}{2016}{{Wu et~al.}}{{}}}
\bibcite{kde}{{45}{2012}{{{Zanin Zambom} and {Dias}}}{{}}}
\bibcite{ADVVI}{{46}{2017}{{Zhang et~al.}}{{}}}
\bibcite{neuroplast}{{47}{1992}{{Zilles}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix A}Proofs}{80}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:A}{{A}{80}{Proofs}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Proof of Proposition 2.6.2}{80}{appendix.A.1}}
\newlabel{app:A.1}{{A.1}{80}{Proof of Proposition 2.6.2}{appendix.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Lemma 3.7.1}{81}{appendix.A.2}}
\newlabel{app:A.2}{{A.2}{81}{Proof of Lemma 3.7.1}{appendix.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of Lemma 4.1.1}{81}{appendix.A.3}}
\newlabel{app:A.3}{{A.3}{81}{Proof of Lemma 4.1.1}{appendix.A.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Proof of Lemma 4.2.2}{82}{appendix.A.4}}
\newlabel{app:A.4}{{A.4}{82}{Proof of Lemma 4.2.2}{appendix.A.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix B}Algorithms}{83}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:B}{{B}{83}{Algorithms}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Back-Propagation Algorithm}{83}{appendix.B.1}}
\newlabel{app:B.1}{{B.1}{83}{Back-Propagation Algorithm}{appendix.B.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Back-Propagation Algorithm\relax }}{83}{algocf.5}}
\newlabel{alg:5}{{5}{83}{Back-Propagation Algorithm}{algocf.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Coordinate Ascent Variational Inference Algorithm}{84}{appendix.B.2}}
\newlabel{app:B.2}{{B.2}{84}{Coordinate Ascent Variational Inference Algorithm}{appendix.B.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Coordinate Ascent Variational Inference (CAVI)\relax }}{84}{algocf.6}}
\newlabel{alg:6}{{6}{84}{Coordinate Ascent Variational Inference Algorithm}{algocf.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Algorithms for ``Sprinkler" Experiment}{85}{appendix.B.3}}
\newlabel{app:B.3}{{B.3}{85}{Algorithms for ``Sprinkler" Experiment}{appendix.B.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Sprinkler Prior-Contrastive Algorithm\relax }}{85}{algocf.7}}
\newlabel{alg:7}{{7}{85}{Algorithms for ``Sprinkler" Experiment}{algocf.7}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Sprinkler Joint-Contrastive Algorithm\relax }}{86}{algocf.8}}
\newlabel{alg:8}{{8}{86}{Algorithms for ``Sprinkler" Experiment}{algocf.8}{}}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix C}Mean Field Variational Inference Example}{87}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:C}{{C}{87}{Mean Field Variational Inference Example}{appendix.C}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces CAVI Algorithm for Bayesian mixture of Gaussians\relax }}{90}{algocf.9}}
\newlabel{alg:9}{{9}{90}{Mean Field Variational Inference Example}{algocf.9}{}}
\citation{kde}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix D}Kernel Density Estimation}{91}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:D}{{D}{91}{Kernel Density Estimation}{appendix.D}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix E}Prior-Contrastive Optimal Estimator Experiment Plots}{92}{appendix.E}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:E}{{E}{92}{Prior-Contrastive Optimal Estimator Experiment Plots}{appendix.E}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.1}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{92}{figure.caption.56}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.2}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{93}{figure.caption.57}}
\@writefile{lof}{\contentsline {figure}{\numberline {E.3}{\ignorespaces Prior-Contrastive $NELBO$\relax }}{93}{figure.caption.58}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix F}Second Functional Derivatives of Direct Log Ratio Estimator Losses}{94}{appendix.F}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:F}{{F}{94}{Second Functional Derivatives of Direct Log Ratio Estimator Losses}{appendix.F}{}}
