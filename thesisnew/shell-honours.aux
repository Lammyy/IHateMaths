\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{gelman}
\citation{blei}
\citation{blei}
\citation{ADVVI}
\citation{kingma}
\citation{mescheder}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{intro}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Problem Context}{1}{section.1.1}}
\citation{sugiyama,mohamed}
\citation{gan}
\citation{nguyen}
\citation{huszar}
\citation{tran}
\citation{kingma}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Aims}{2}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Results}{3}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Thesis Structure}{4}{section.1.4}}
\citation{DeepLearning}
\citation{neuroplast}
\citation{neuroplast}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 2}Background on Neural Networks}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch2}{{2}{5}{Background on Neural Networks}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{5}{section.2.1}}
\newlabel{sec:2.1}{{2.1}{5}{Motivation}{section.2.1}{}}
\citation{DeepLearning}
\citation{universal,cybenko}
\citation{mnist}
\citation{neuralstat}
\citation{haykin}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Individual Node Structure}{6}{section.2.2}}
\newlabel{sec:2.2}{{2.2}{6}{Individual Node Structure}{section.2.2}{}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example structure of an individual node function with 3 inputs, labelled as $\bm  {x}=[x_0\hskip 1em\relax x_1\hskip 1em\relax x_2\hskip 1em\relax x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm  {\theta }=[\theta _0\hskip 1em\relax \theta _1\hskip 1em\relax \theta _2\hskip 1em\relax \theta _3]$.\relax }}{7}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2.1}{{2.1}{7}{\small Example structure of an individual node function with 3 inputs, labelled as $\bm {x}=[x_0\quad x_1\quad x_2\quad x_3]^\top $, with $x_0$ corresponding to the bias node. The weights are denoted as $\bm {\theta }=[\theta _0\quad \theta _1\quad \theta _2\quad \theta _3]$.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Activation Functions}{7}{section.2.3}}
\newlabel{sec:2.3}{{2.3}{7}{Activation Functions}{section.2.3}{}}
\citation{snyman}
\citation{wu}
\citation{cybenko}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Activation Function Plots\relax }}{8}{figure.caption.7}}
\newlabel{fig:2.2}{{2.2}{8}{Activation Function Plots\relax }{figure.caption.7}{}}
\citation{neuralstat}
\citation{DeepLearning}
\citation{sparse}
\citation{kolen}
\citation{sparse}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Neural Network Structure}{9}{section.2.4}}
\newlabel{sec:2.4}{{2.4}{9}{Neural Network Structure}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Example of a neural network structure with 3 external inputs, 1 hidden layer (with 3 nodes), 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in \autoref  {sec:2.2}: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm  {\Theta }$ denotes the weights of the network.\relax }}{10}{figure.caption.8}}
\newlabel{fig:2.3}{{2.3}{10}{\small Example of a neural network structure with 3 external inputs, 1 hidden layer (with 3 nodes), 1 bias node per non-output layer and 1 output node. The variable inside each node denotes it's output as calculated in \autoref {sec:2.2}: the output of node $i$ in layer $j$ is denoted as $a_i^{(j)}$. $\bm {\Theta }$ denotes the weights of the network.\relax }{figure.caption.8}{}}
\citation{bishop}
\citation{xavier}
\citation{goodman}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Weight Initialization}{11}{section.2.5}}
\newlabel{sec:2.5}{{2.5}{11}{Weight Initialization}{section.2.5}{}}
\citation{DeepLearning}
\citation{optimneural}
\citation{optim}
\citation{beck}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Optimization}{12}{section.2.6}}
\newlabel{sec:2.6}{{2.6}{12}{Optimization}{section.2.6}{}}
\citation{floudas}
\citation{batch,optimneural}
\citation{bengio}
\citation{optimneural}
\citation{adam}
\citation{backprop}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Back-Propagation}{14}{section.2.7}}
\newlabel{sec:2.7}{{2.7}{14}{Back-Propagation}{section.2.7}{}}
\citation{gelman}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 3}Variational Inference}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch3}{{3}{15}{Variational Inference}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Context}{15}{section.3.1}}
\newlabel{sec:3.1}{{3.1}{15}{Context}{section.3.1}{}}
\citation{KL}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}The KL Divergence}{16}{section.3.2}}
\newlabel{sec:3.2}{{3.2}{16}{The KL Divergence}{section.3.2}{}}
\newlabel{def:3.2.1}{{3.2.1}{16}{}{theorem.3.2.1}{}}
\citation{blei}
\newlabel{3.2.5}{{3.2.5}{17}{}{theorem.3.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Introduction to Variational Inference}{17}{section.3.3}}
\newlabel{sec:3.3}{{3.3}{17}{Introduction to Variational Inference}{section.3.3}{}}
\newlabel{eqn:3.3.1}{{3.3.1}{17}{Introduction to Variational Inference}{equation.3.3.1}{}}
\newlabel{eqn:3.3.2}{{3.3.2}{17}{Introduction to Variational Inference}{equation.3.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Derivation of the ELBO}{17}{section.3.4}}
\newlabel{sec:3.4}{{3.4}{17}{Derivation of the ELBO}{section.3.4}{}}
\citation{blei}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Mean-Field Variational Family}{18}{section.3.5}}
\newlabel{sec:3.5}{{3.5}{18}{Mean-Field Variational Family}{section.3.5}{}}
\citation{blei}
\citation{ADVVI}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Amortized Inference}{19}{section.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm  {\phi ^{(i)}}$ corresponding to each latent variable point $\bm  {z}^{(i)}$ has to be found. $\bm  {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }}{20}{figure.caption.9}}
\newlabel{fig:3.1}{{3.1}{20}{\small This DAG represents Mean-Field Variational Inference. For each of the $K$ datasets, a set of parameter sets $\bm {\phi ^{(i)}}$ corresponding to each latent variable point $\bm {z}^{(i)}$ has to be found. $\bm {\phi }^{(i)}$ is $M$-dimensional, so there is a total of $M\times K$ sets of parameters.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm  {\phi }$, and each data point $\bm  {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm  {\phi }}(\bm  {z}|\bm  {x})$ to find $\bm  {z}^{(i)}$.\relax }}{20}{figure.caption.10}}
\newlabel{fig:3.2}{{3.2}{20}{\small This DAG represents Amortized Variational Inference. Here, there is only one set of variational parameters $\bm {\phi }$, and each data point $\bm {x}^{(i)}$ is used as an input in the variational posterior $q_{\bm {\phi }}(\bm {z}|\bm {x})$ to find $\bm {z}^{(i)}$.\relax }{figure.caption.10}{}}
\citation{kingma}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Example: Variational Autoencoder}{21}{section.3.7}}
\newlabel{sec:3.7}{{3.7}{21}{Example: Variational Autoencoder}{section.3.7}{}}
\citation{vae}
\citation{mescheder}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm  {x}$ is passed through the encoder $q_\phi (\bm  {z}|\bm  {x})$ to produce a mean vector $\bm  {\mu }$ and a variance vector $\bm  {\sigma }^2$. Random noise $\bm  {\varepsilon }_1$ is sampled from $\mathcal  {N}(0,I_{M\times M})$ and transformed to generate $\bm  {z}$: $\bm  {z}=\bm  {\mu }+\bm  {\varepsilon }\cdot \bm  {\sigma }^2$. This latent variable $\bm  {z}$ is passed through the decoder $p_\theta (\bm  {x}|\bm  {z})$ to reconstruct the data point as $\mathaccentV {tilde}07E{\bm  {x}}$. Note that random noise $\bm  {\varepsilon }_2$ is not a direct input into the decoder, rather it represents the noise density added to make the neural network probabilistic.\relax }}{23}{figure.caption.11}}
\newlabel{fig:3.3}{{3.3}{23}{\small A simple DAG representing a Variational Autoencoder. An arbitrary data point $\bm {x}$ is passed through the encoder $q_\phi (\bm {z}|\bm {x})$ to produce a mean vector $\bm {\mu }$ and a variance vector $\bm {\sigma }^2$. Random noise $\bm {\varepsilon }_1$ is sampled from $\mathcal {N}(0,I_{M\times M})$ and transformed to generate $\bm {z}$: $\bm {z}=\bm {\mu }+\bm {\varepsilon }\cdot \bm {\sigma }^2$. This latent variable $\bm {z}$ is passed through the decoder $p_\theta (\bm {x}|\bm {z})$ to reconstruct the data point as $\tilde {\bm {x}}$. Note that random noise $\bm {\varepsilon }_2$ is not a direct input into the decoder, rather it represents the noise density added to make the neural network probabilistic.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip To generate new data $\mathaccentV {tilde}07E{\bm  {x}}$ similar to existing data $\bm  {x}$, we sample latent variable $\bm  {z}$ from the prior density $p(\bm  {z})$ and pass it through the decoder $p_\theta (\bm  {x}|\bm  {z})$.\relax }}{23}{figure.caption.12}}
\newlabel{fig:3.4}{{3.4}{23}{\small To generate new data $\tilde {\bm {x}}$ similar to existing data $\bm {x}$, we sample latent variable $\bm {z}$ from the prior density $p(\bm {z})$ and pass it through the decoder $p_\theta (\bm {x}|\bm {z})$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Problems with Implicit Densities}{23}{section.3.8}}
\newlabel{sec:3.8}{{3.8}{23}{Problems with Implicit Densities}{section.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Implicit Prior and/or Variational Posterior}{23}{subsection.3.8.1}}
\newlabel{sec:3.8.1}{{3.8.1}{23}{Implicit Prior and/or Variational Posterior}{subsection.3.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\varepsilon _1$ according to the mean vector $\bm  {\mu }$ and variance vector $\bm  {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal  {G}_\phi (\varepsilon _1;\bm  {x})$ directly as an additional input. The likelihood density $p_\theta (\bm  {x}|\bm  {z})$ has the same explicit representation as in Figures \ref  {fig:3.3} and \ref  {fig:3.4}.\relax }}{24}{figure.caption.13}}
\newlabel{fig:3.5}{{3.5}{24}{\small This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder. Rather than transforming random noise $\varepsilon _1$ according to the mean vector $\bm {\mu }$ and variance vector $\bm {\sigma }$ output of the variational posterior $q_\phi (z|x)$, we add the noise to the encoder network $\mathcal {G}_\phi (\varepsilon _1;\bm {x})$ directly as an additional input. The likelihood density $p_\theta (\bm {x}|\bm {z})$ has the same explicit representation as in Figures \ref {fig:3.3} and \ref {fig:3.4}.\relax }{figure.caption.13}{}}
\citation{ali}
\citation{tran}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.2}Implicit Likelihood}{25}{subsection.3.8.2}}
\newlabel{sec:3.8.2}{{3.8.2}{25}{Implicit Likelihood}{subsection.3.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts a variation of ``Adversarial Variational Bayes", in which random noise $\bm  {\varepsilon }_2$ is additionally added to the input of the generator of likelihood density samples $G_\theta (\bm  {\varepsilon }_2;\bm  {x})$. The likelihood density $p_\theta (\bm  {x}|\bm  {z})$ is therefore implicit and consequently, its corresponding term in the optimization problem cannot be evaluated.\relax }}{25}{figure.caption.14}}
\newlabel{fig:3.6}{{3.6}{25}{\small This diagram depicts a variation of ``Adversarial Variational Bayes", in which random noise $\bm {\varepsilon }_2$ is additionally added to the input of the generator of likelihood density samples $G_\theta (\bm {\varepsilon }_2;\bm {x})$. The likelihood density $p_\theta (\bm {x}|\bm {z})$ is therefore implicit and consequently, its corresponding term in the optimization problem cannot be evaluated.\relax }{figure.caption.14}{}}
\citation{tiao}
\citation{sugiyama,mohamed}
\citation{huszar}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 4}Density Ratio Estimation}{27}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch4}{{4}{27}{Density Ratio Estimation}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Class Probability Estimation}{27}{section.4.1}}
\newlabel{sec:4.1}{{4.1}{27}{Class Probability Estimation}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Derivation}{27}{subsection.4.1.1}}
\newlabel{sec:4.1.1}{{4.1.1}{27}{Derivation}{subsection.4.1.1}{}}
\citation{sugiyama}
\citation{gan}
\newlabel{rem:4.1.2}{{4.1.2}{28}{}{theorem.4.1.2}{}}
\newlabel{rem:4.1.3}{{4.1.3}{28}{}{theorem.4.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Prior-Contrastive Algorithm}{29}{subsection.4.1.2}}
\newlabel{sec:4.1.2}{{4.1.2}{29}{Prior-Contrastive Algorithm}{subsection.4.1.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Prior-Contrastive Class Probability Estimation\relax }}{31}{algocf.1}}
\newlabel{alg:1}{{1}{31}{Prior-Contrastive Algorithm}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Joint-Contrastive Algorithm}{31}{subsection.4.1.3}}
\newlabel{sec:4.1.3}{{4.1.3}{31}{Joint-Contrastive Algorithm}{subsection.4.1.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Joint-Contrastive Class Probability Estimation\relax }}{33}{algocf.2}}
\newlabel{alg:2}{{2}{33}{Joint-Contrastive Algorithm}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Divergence Minimization}{33}{section.4.2}}
\newlabel{sec:4.2}{{4.2}{33}{Divergence Minimization}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Derivation}{33}{subsection.4.2.1}}
\newlabel{sec:4.2.1}{{4.2.1}{33}{Derivation}{subsection.4.2.1}{}}
\citation{nguyen}
\citation{nguyen}
\newlabel{4.2.1}{{4.2.2}{34}{}{theorem.4.2.2}{}}
\newlabel{rem:4.2.4}{{4.2.5}{35}{}{theorem.4.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Prior-Contrastive Algorithm}{35}{subsection.4.2.2}}
\newlabel{sec:4.2.2}{{4.2.2}{35}{Prior-Contrastive Algorithm}{subsection.4.2.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Prior-Contrastive Divergence Minimization\relax }}{36}{algocf.3}}
\newlabel{alg:3}{{3}{36}{Prior-Contrastive Algorithm}{algocf.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Joint-Contrastive Algorithm}{37}{subsection.4.2.3}}
\newlabel{sec:4.2.3}{{4.2.3}{37}{Joint-Contrastive Algorithm}{subsection.4.2.3}{}}
\citation{tiao}
\citation{JS}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces Joint-Contrastive Divergence Minimization\relax }}{38}{algocf.4}}
\newlabel{alg:4}{{4}{38}{Joint-Contrastive Algorithm}{algocf.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Alternative Derivation of Class Probability Estimation}{38}{subsection.4.2.4}}
\newlabel{sec:4.2.4}{{4.2.4}{38}{Alternative Derivation of Class Probability Estimation}{subsection.4.2.4}{}}
\citation{gan}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 5}Algorithm Generalization}{41}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch5}{{5}{41}{Algorithm Generalization}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{41}{section.5.1}}
\newlabel{sec:5.1}{{5.1}{41}{Introduction}{section.5.1}{}}
\newlabel{eq:5.1.1}{{5.1.1}{41}{Introduction}{equation.5.1.1}{}}
\newlabel{eq:5.1.2}{{5.1.2}{42}{Introduction}{equation.5.1.2}{}}
\newlabel{eq:5.1.3}{{5.1.3}{42}{}{equation.5.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Algorithm Generalization}{42}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Reverse KL Divergence}{43}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}GAN Divergence}{43}{subsection.5.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Optimization Algorithms}{44}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Prior-Contrastive Loss Functions}{44}{subsection.5.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Joint-Contrastive Loss Functions}{44}{subsection.5.3.2}}
\citation{nowozin}
\citation{huszar}
\citation{explain}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 6}Comparing Optimal Estimators}{46}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch6}{{6}{46}{Comparing Optimal Estimators}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Theory}{46}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Problem Context}{46}{section.6.2}}
\citation{gan}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly bi-modal and unusually shaped.\relax }}{48}{figure.caption.19}}
\newlabel{fig:6.1}{{6.1}{48}{\small This figure depicts the true (unnormalised) posterior plots for the ``Continuous Sprinkler" experiment. As $x$ increases, the posteriors become increasingly bi-modal and unusually shaped.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Program Structure}{48}{section.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\varepsilon })$ used in the ``Continuous Sprinkler" experiment. We do not use an activation function for the output layer as $\bm  {z}\in \mathbb  {R}^2$.\relax }}{48}{figure.caption.20}}
\newlabel{fig:6.2}{{6.2}{48}{\small This figure illustrates the structure of the generator network $\mathcal {G}(x,\bm {\varepsilon })$ used in the ``Continuous Sprinkler" experiment. We do not use an activation function for the output layer as $\bm {z}\in \R ^2$.\relax }{figure.caption.20}{}}
\citation{DeepLearning}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in \autoref  {fig:6.2}. Note that the only differences between the estimators are the activation function of their output layers and the loss functions used to train them.\relax }}{49}{figure.caption.21}}
\newlabel{fig:6.3}{{6.3}{49}{\small This figure depicts the structure of the estimator network for the ``Continuous Sprinkler Experiment". The notation is the same as in \autoref {fig:6.2}. Note that the only differences between the estimators are the activation function of their output layers and the loss functions used to train them.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Results}{50}{section.6.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we compare the true (unnormalised) posterior as plotted in \autoref  {fig:6.1} against variational posterior outputs corresponding to varying KL divergences. Sub-figures (b)-(d) were created by fitting a Gaussian kernel density estimation over 1000 posterior samples. Note that the variational posterior is relatively optimal at a `true' KL divergence of 1.3288, and that the output appears less flexible as the divergence increases. In sub-figure (d), the variational posterior fails to capture the bimodality of the true posterior.\relax }}{51}{figure.caption.22}}
\newlabel{fig:6.4}{{6.4}{51}{\small Above we compare the true (unnormalised) posterior as plotted in \autoref {fig:6.1} against variational posterior outputs corresponding to varying KL divergences. Sub-figures (b)-(d) were created by fitting a Gaussian kernel density estimation over 1000 posterior samples. Note that the variational posterior is relatively optimal at a `true' KL divergence of 1.3288, and that the output appears less flexible as the divergence increases. In sub-figure (d), the variational posterior fails to capture the bimodality of the true posterior.\relax }{figure.caption.22}{}}
\citation{nowozin}
\citation{nowozin}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This table presents the results from the prior-contrastive experiment. Since the programs reached optimality in this case, it is uncertain from those results whether the choice of $f$-divergence or estimator parametrization impacts posterior convergence.\relax }}{52}{table.caption.23}}
\newlabel{tab:6.1}{{6.1}{52}{\small This table presents the results from the prior-contrastive experiment. Since the programs reached optimality in this case, it is uncertain from those results whether the choice of $f$-divergence or estimator parametrization impacts posterior convergence.\relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip From these joint-contrastive results, it is evident that for each $f$-divergence, similar posterior convergence is associated with the different estimator parametrizations. As ``$f$-GAN: Training Generative Neural Samplers using Variational Divergence Minimization" by \citet  {nowozin} suggests, the GAN divergence demonstrates slower and more inconsistent results than the reverse KL divergence.\relax }}{52}{table.caption.24}}
\newlabel{tab:6.2}{{6.2}{52}{\small From these joint-contrastive results, it is evident that for each $f$-divergence, similar posterior convergence is associated with the different estimator parametrizations. As ``$f$-GAN: Training Generative Neural Samplers using Variational Divergence Minimization" by \citet {nowozin} suggests, the GAN divergence demonstrates slower and more inconsistent results than the reverse KL divergence.\relax }{table.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These plots of the true KL divergence in the joint-contrastive context show near identical convergence for all estimators.\relax }}{52}{figure.caption.25}}
\newlabel{fig:6.5}{{6.5}{52}{\small These plots of the true KL divergence in the joint-contrastive context show near identical convergence for all estimators.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In sub-figure (a), the estimators have similar levels of stability, but the class probability estimator loss appears to be higher than the other two estimator parametrizations. This does not appear to affect posterior convergence. All three estimators formulated by the reverse KL divergence have similar, increasingly unstable losses.\relax }}{53}{figure.caption.26}}
\newlabel{fig:6.6}{{6.6}{53}{\small In sub-figure (a), the estimators have similar levels of stability, but the class probability estimator loss appears to be higher than the other two estimator parametrizations. This does not appear to affect posterior convergence. All three estimators formulated by the reverse KL divergence have similar, increasingly unstable losses.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These joint-contrastive $NELBO$ plots generally mirror the estimator loss trends depicted in \autoref  {fig:6.6}. Lower and more stable $NELBO$ estimations are associated with the reverse KL divergence, correlating with its superior posterior convergence. It is unclear why the $NELBO$ increases over the iterations when its minimization theoretically leads to posterior convergence.\relax }}{53}{figure.caption.27}}
\newlabel{fig:6.7}{{6.7}{53}{\small These joint-contrastive $NELBO$ plots generally mirror the estimator loss trends depicted in \autoref {fig:6.6}. Lower and more stable $NELBO$ estimations are associated with the reverse KL divergence, correlating with its superior posterior convergence. It is unclear why the $NELBO$ increases over the iterations when its minimization theoretically leads to posterior convergence.\relax }{figure.caption.27}{}}
\citation{lecun}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 7}Comparing Undertrained Estimators}{54}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch7}{{7}{54}{Comparing Undertrained Estimators}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Theory}{54}{section.7.1}}
\newlabel{sec:7.1}{{7.1}{54}{Theory}{section.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Estimator Bounds}{54}{subsection.7.1.1}}
\citation{lecun}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}First and Second Derivatives of Estimator Loss Functions}{55}{subsection.7.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Above we have plotted the estimator output against its loss functional. The same $x$ and $y$-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }}{56}{figure.caption.28}}
\newlabel{fig:7.1}{{7.1}{56}{\small Above we have plotted the estimator output against its loss functional. The same $x$ and $y$-axis scales are used for all plots. Due to the bounds, it is evident that the loss functionals using the class probability estimator $D_\alpha (u)$ are associated with the highest gradient values and therefore the fastest convergence. It is difficult to compare the direct ratio estimator $r_\alpha (u)$ and the direct log ratio estimator $T_\alpha (u)$, as the former estimator appears to have higher gradients when $q(u)<p(u)$, but slower convergence when $q(u)>p(u)$. Since the relative gradients of the graph can vary with the choice of $p(u)$ and $q(u)$, we cannot compare the $f$-divergences.\relax }{figure.caption.28}{}}
\citation{nowozin}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Displacement of Estimator Optimal Values}{57}{subsection.7.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Experiment Outline}{59}{section.7.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Results}{59}{section.7.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these prior-contrastive results, it is evident that using the reverse KL divergence to derive the lower bound for the estimator loss function leads to higher posterior convergence, supporting our experimental results in \autoref  {ch6}.\relax }}{60}{table.caption.31}}
\newlabel{tab:7.1}{{7.1}{60}{\small In these prior-contrastive results, it is evident that using the reverse KL divergence to derive the lower bound for the estimator loss function leads to higher posterior convergence, supporting our experimental results in \autoref {ch6}.\relax }{table.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the reduction in the true average KL divergence over the prior contrastive experiments' runtime. Although the direct ratio estimator plots initially experience a faster reduction, all three estimator plots meet at approximately the same KL divergence. It can be seen that the GAN divergence is associated with a faster initial drop than the reverse KL divergence, yet experiences lower overall posterior convergence.\relax }}{60}{figure.caption.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these prior-contrastive estimator loss plots, it is evident that the estimators bound by the reverse KL divergence experience initial fluctuations which eventually stabilize, whilst the GAN divergence is associated with higher stability. This is consistent with the observations made in Figure 7.2, and implies a trade-off between estimator stability and accuracy in the choice of $f$-divergence.\relax }}{61}{figure.caption.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The trends shown in the $NELBO$ plots above are generally consistent with the corresponding estimator loss plots. All of the estimators converge at approximately the same estimated $NELBO$, indicating that the effects of under-training the estimator are mostly experienced in the early program iterations.\relax }}{61}{figure.caption.34}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these joint-contrastive results, there is a considerable difference between the three estimators when the reverse KL bound is used. However, when the GAN divergence is used to formulate the estimator loss function, only the direct log ratio estimator demonstrates substantially worse posterior convergence than the other two estimators.\relax }}{62}{table.caption.35}}
\newlabel{tab:7.2}{{7.2}{62}{\small In these joint-contrastive results, there is a considerable difference between the three estimators when the reverse KL bound is used. However, when the GAN divergence is used to formulate the estimator loss function, only the direct log ratio estimator demonstrates substantially worse posterior convergence than the other two estimators.\relax }{table.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Similar to the true KL divergence plots in Figure 7.2, these joint-contrastive plots show that the estimators formulated with the GAN divergence experience faster initial convergence. However, this is unreliable as the reverse KL divergence experiments appear to initialize at a much higher true KL divergence.\relax }}{62}{figure.caption.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In these estimator loss plots, we again see that the reverse KL divergence plot demonstrates initial estimator instability. This is in stark contrast to the relatively stable GAN divergence plot. From sub-figure (a), we note that the direct ratio estimator is more unstable and has a lower loss value than the other two estimators. The cause of this is unknown, as the direct ratio estimator leads to similar posterior convergence to the more consistent class probability estimator.\relax }}{63}{figure.caption.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The joint-contrastive estimated $NELBO$ plots above generally have similar trends to the corresponding estimator plots. Here we note that the final $NELBO$ value of the reverse KL divergence experiments is lower than that of the GAN divergence. However, this may be due to its initial fluctuations delaying the start of the increasing $NELBO$ trend by approximately 250 iterations.\relax }}{63}{figure.caption.38}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 8}Autoencoder Experiment - (MNIST Dataset)}{64}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch8}{{8}{64}{Autoencoder Experiment - (MNIST Dataset)}{chapter.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Experiment Outline}{64}{section.8.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Samples from the MNIST Dataset\relax }}{64}{figure.caption.39}}
\newlabel{fig:8.1}{{8.1}{64}{Samples from the MNIST Dataset\relax }{figure.caption.39}{}}
\citation{nowozin,bgan}
\citation{nowozin,bgan}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure illustrates the structure of the generator network $\mathcal  {G}(x,\bm  {\varepsilon })$ used in the MNIST data generation experiment. The dimensionality of latent variable $z$ is either 2 or 20 depending on the experimental setting. We have arbitrarily chosen the number of random noise inputs $\varepsilon $ to be 4.\relax }}{65}{figure.caption.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This is a diagram of the decoder network used in the MNIST data generation experiment. A sigmoid output layer is used to map the network output to $(0,1)$, suiting the grayscale nature of the data. Although this problem involves image analysis, we refrain from using convolutional layers due to the relatively small image size. This is consistent with other similar MNIST experiments \citep  {nowozin, bgan}.\relax }}{65}{figure.caption.41}}
\citation{nowozin,bgan,tiao}
\citation{nowozin}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This figure depicts the estimator network for the MNIST data generation experiment. Depending on the estimator parametrization, the output activation function is either sigmoid for the class probability estimator $D_\alpha (z,x)\simeq q_\phi (z|x)/(q_\phi (z|x)+p(z))$, exponential for the direct ratio estimator $r_\alpha (z,x)\simeq q_\phi (z|x)/p(z)$ or linear for the direct log ratio estimator $T_\alpha (z,x)\simeq \qopname  \relax o{log}(q_\phi (z|x)/p(z))$.\relax }}{66}{figure.caption.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This diagram depicts the ``Adversarial Variational Bayes" formulation of the variational autoencoder used in the MNIST data generation experiment. Note that there is no random noise added to the decoder.\relax }}{67}{figure.caption.43}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Low Dimensional Experiment Results}{67}{section.8.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The low-dimensional MNIST autoencoder experiment results are tabulated above. We note that the relative performances of the different estimator loss functions is the same as in \autoref  {ch7}.\relax }}{67}{table.caption.44}}
\citation{bgan}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip These figures exemplify data reconstruction corresponding to different reconstruction errors. The first two rows of each sub-figure depict example data input $x$, and the last two rows show the reconstructed output $\mathaccentV {tilde}07E{x}$.\relax }}{68}{figure.caption.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip As shown in the above average reconstruction error plots, the direct log ratio estimator in sub-figure (a) consistently has a higher reconstruction error.\relax }}{68}{figure.caption.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Noting the scale of the estimator loss plot in sub-figure (b), it is evident that the reverse KL divergence leads to unstable estimator training. Unlike the estimators plotted in Figure 7.3 (b), these estimators don't appear to stabilize after a certain period. Sub-figure (a) shows that the direct log ratio estimator loss is consistently higher than the other two estimators, correlating with its poorer reconstruction.\relax }}{69}{figure.caption.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Despite the apparent instability of the estimators trained with the reverse KL divergence, the $NELBO$ plot in sub-figure (b) is relatively consistent, leading to a smooth convergence as plotted in Figure 8.7 (b). The high direct log ratio estimator loss associated with the GAN divergence corresponds to a relatively large $NELBO$, as depicted in sub-figure (a).\relax }}{69}{figure.caption.48}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}High Dimensional Experiment Results}{69}{section.8.3}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip In the high dimensional MNIST autoencoder experiment results, it is evident that the GAN divergence correlates with substantially lower reconstruction error mean and standard deviation, contradicting the superiority of the reverse KL divergence shown in previous experiments.\relax }}{71}{table.caption.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Similar to Figure 8.6, these figures exemplify data reconstruction corresponding to different reconstruction errors.\relax }}{71}{figure.caption.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip This plot shows that the reconstruction error associated with the reverse KL divergence is consistently higher for the majority of the program runtime.\relax }}{72}{figure.caption.51}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The estimator loss corresponding to the reverse KL divergence appears to be extremely unstable for the entire program runtime, spiking to values exceeding $10^{14}$. On the other hand, the GAN divergence estimator loss appears to be relatively stable.\relax }}{72}{figure.caption.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces \relax \fontsize  {10.95}{13.6}\selectfont  \abovedisplayskip 11\p@ plus3\p@ minus6\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6.5\p@ plus3.5\p@ minus3\p@ \def \leftmargin \leftmargini \labelsep .5em\labelwidth \leftmargini \advance \labelwidth -\labelsep \parsep \z@ \topsep 0.4ex plus\p@ \itemsep 0\p@ plus1\p@ {\leftmargin \leftmargini \topsep 9\p@ plus3\p@ minus5\p@ \parsep 4.5\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip The instability of the reverse KL divergence estimator loss propagates to its $NELBO$ estimation, which appears to fluctuate wildly. This correlates to the superiority of the GAN divergence in this particular experiment. It appears that the reverse KL estimator in this experiment has not stabilized, leading to poorer network convergence.\relax }}{72}{figure.caption.53}}
\citation{nowozin}
\@writefile{toc}{\contentsline {chapter}{\numberline {Chapter 9}Conclusion and Further Research}{73}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch9}{{9}{73}{Conclusion and Further Research}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Further Research}{73}{section.9.1}}
\citation{bgan}
\citation{ali}
\citation{wang}
\citation{vincent}
\citation{ais}
\bibstyle{apalike}
\bibdata{bible}
\bibcite{beck}{{1}{2014}{{Beck}}{{}}}
\bibcite{bengio}{{2}{2012}{{Bengio}}{{}}}
\bibcite{bishop}{{3}{1995}{{Bishop}}{{}}}
\bibcite{pattern}{{4}{2006}{{Bishop}}{{}}}
\bibcite{blei}{{5}{2017}{{Blei et~al.}}{{}}}
\bibcite{optim}{{6}{2004}{{Boyd and Vandenberghe}}{{}}}
\bibcite{neuralstat}{{7}{1994}{{Cheng and Titterington}}{{}}}
\bibcite{cybenko}{{8}{1989}{{Cybenko}}{{}}}
\bibcite{vae}{{9}{2016}{{Doersch}}{{}}}
\bibcite{ali}{{10}{2016}{{{Dumoulin} et~al.}}{{}}}
\bibcite{backprop}{{11}{1986}{{E.~Rumelhart et~al.}}{{}}}
\bibcite{floudas}{{12}{2005}{{Floudas}}{{}}}
\bibcite{JS}{{13}{2004}{{Fuglede and Topsoe}}{{}}}
\@writefile{toc}{\contentsline {chapter}{References}{75}{section.9.1}}
\bibcite{gelman}{{14}{2004}{{Gelman et~al.}}{{}}}
\bibcite{xavier}{{15}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{sparse}{{16}{2011}{{Glorot et~al.}}{{}}}
\bibcite{DeepLearning}{{17}{2016}{{Goodfellow et~al.}}{{}}}
\bibcite{gan}{{18}{2014}{{Goodfellow et~al.}}{{}}}
\bibcite{goodman}{{19}{1960}{{Goodman}}{{}}}
\bibcite{haykin}{{20}{1998}{{Haykin}}{{}}}
\bibcite{universal}{{21}{1991}{{Hornik}}{{}}}
\bibcite{huszar}{{22}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{adam}{{23}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kingma}{{24}{2013}{{{Kingma} and {Welling}}}{{}}}
\bibcite{kolen}{{25}{2001}{{Kolen and Kremer}}{{}}}
\bibcite{KL}{{26}{1959}{{Kullback}}{{}}}
\bibcite{lecun}{{27}{2012}{{LeCun et~al.}}{{}}}
\bibcite{batch}{{28}{2014}{{Li et~al.}}{{}}}
\bibcite{mescheder}{{29}{2017}{{Mescheder et~al.}}{{}}}
\bibcite{mohamed}{{30}{2016}{{{Mohamed} and {Lakshminarayanan}}}{{}}}
\bibcite{nguyen}{{31}{2010}{{Nguyen et~al.}}{{}}}
\bibcite{nowozin}{{32}{2016}{{Nowozin et~al.}}{{}}}
\bibcite{optimneural}{{33}{2016}{{Ruder}}{{}}}
\bibcite{mnist}{{34}{2003}{{Simard et~al.}}{{}}}
\bibcite{snyman}{{35}{2005}{{Snyman}}{{}}}
\bibcite{sugiyama}{{36}{2012}{{Sugiyama et~al.}}{{}}}
\bibcite{tiao}{{37}{2018}{{Tiao et~al.}}{{}}}
\bibcite{tran}{{38}{2017}{{Tran et~al.}}{{}}}
\bibcite{bgan}{{39}{2016}{{{Uehara} et~al.}}{{}}}
\bibcite{vincent}{{40}{2008}{{Vincent et~al.}}{{}}}
\bibcite{wang}{{41}{2009}{{Wang et~al.}}{{}}}
\bibcite{explain}{{42}{1993}{{Wellman and Henrion}}{{}}}
\bibcite{wu}{{43}{2009}{{Wu}}{{}}}
\bibcite{ais}{{44}{2016}{{Wu et~al.}}{{}}}
\bibcite{kde}{{45}{2012}{{{Zanin Zambom} and {Dias}}}{{}}}
\bibcite{ADVVI}{{46}{2017}{{Zhang et~al.}}{{}}}
\bibcite{neuroplast}{{47}{1992}{{Zilles}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix A}Proofs}{79}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:A}{{A}{79}{Proofs}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Proof of Proposition 2.6.2}{79}{appendix.A.1}}
\newlabel{app:A.1}{{A.1}{79}{Proof of Proposition 2.6.2}{appendix.A.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Proof of Lemma 3.2.4}{80}{appendix.A.2}}
\newlabel{app:swag}{{A.2}{80}{Proof of Lemma 3.2.4}{appendix.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Proof of Lemma 3.2.5}{80}{appendix.A.3}}
\newlabel{app:asdf}{{A.3}{80}{Proof of Lemma 3.2.5}{appendix.A.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Proof of Lemma 3.7.1}{80}{appendix.A.4}}
\newlabel{app:A.2}{{A.4}{80}{Proof of Lemma 3.7.1}{appendix.A.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Proof of Lemma 4.1.1}{81}{appendix.A.5}}
\newlabel{app:A.3}{{A.5}{81}{Proof of Lemma 4.1.1}{appendix.A.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.6}Proof of Lemma 4.2.2}{81}{appendix.A.6}}
\newlabel{app:A.4}{{A.6}{81}{Proof of Lemma 4.2.2}{appendix.A.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.7}Proof of Lemma 8.3.1}{82}{appendix.A.7}}
\newlabel{app:A.5}{{A.7}{82}{Proof of Lemma 8.3.1}{appendix.A.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix B}Algorithms}{83}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:B}{{B}{83}{Algorithms}{appendix.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.1}Back-Propagation Algorithm}{83}{appendix.B.1}}
\newlabel{app:B.1}{{B.1}{83}{Back-Propagation Algorithm}{appendix.B.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {5}{\ignorespaces Back-Propagation Algorithm\relax }}{83}{algocf.5}}
\newlabel{alg:5}{{5}{83}{Back-Propagation Algorithm}{algocf.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.2}Coordinate Ascent Variational Inference Algorithm}{84}{appendix.B.2}}
\newlabel{app:B.2}{{B.2}{84}{Coordinate Ascent Variational Inference Algorithm}{appendix.B.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {6}{\ignorespaces Coordinate Ascent Variational Inference (CAVI)\relax }}{84}{algocf.6}}
\newlabel{alg:6}{{6}{84}{Coordinate Ascent Variational Inference Algorithm}{algocf.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.3}Algorithms for ``Sprinkler" Experiment}{85}{appendix.B.3}}
\newlabel{app:B.3}{{B.3}{85}{Algorithms for ``Sprinkler" Experiment}{appendix.B.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {7}{\ignorespaces Sprinkler Prior-Contrastive Algorithm\relax }}{85}{algocf.7}}
\newlabel{alg:7}{{7}{85}{Algorithms for ``Sprinkler" Experiment}{algocf.7}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {8}{\ignorespaces Sprinkler Joint-Contrastive Algorithm\relax }}{86}{algocf.8}}
\newlabel{alg:8}{{8}{86}{Algorithms for ``Sprinkler" Experiment}{algocf.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B.4}Algorithm for MNIST Experiment}{87}{appendix.B.4}}
\newlabel{app:B.4}{{B.4}{87}{Algorithm for MNIST Experiment}{appendix.B.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {9}{\ignorespaces MNIST Prior-Contrastive Algorithm\relax }}{87}{algocf.9}}
\newlabel{alg:mnist}{{9}{87}{Algorithm for MNIST Experiment}{algocf.9}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix C}Coordinate Ascent Variational Inference Derivation}{88}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:mfvi}{{C}{88}{Coordinate Ascent Variational Inference Derivation}{appendix.C}{}}
\newlabel{mfvi2}{{C.0.1}{88}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.1}{}}
\newlabel{mfvi3}{{C.0.2}{88}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.2}{}}
\citation{pattern}
\newlabel{mfvi4}{{C.0.3}{89}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.3}{}}
\newlabel{mfvi5}{{C.0.4}{89}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.4}{}}
\newlabel{mfvi6}{{C.0.5}{89}{Coordinate Ascent Variational Inference Derivation}{equation.C.0.5}{}}
\citation{blei}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix D}Mean Field Variational Inference Example}{90}{appendix.D}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:C}{{D}{90}{Mean Field Variational Inference Example}{appendix.D}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {10}{\ignorespaces CAVI Algorithm for Bayesian mixture of Gaussians\relax }}{93}{algocf.10}}
\newlabel{alg:9}{{10}{93}{Mean Field Variational Inference Example}{algocf.10}{}}
\citation{kde}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix E}Kernel Density Estimation}{94}{appendix.E}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:D}{{E}{94}{Kernel Density Estimation}{appendix.E}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix F}Prior-Contrastive Optimal Estimator Experiment Plots}{95}{appendix.F}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:E}{{F}{95}{Prior-Contrastive Optimal Estimator Experiment Plots}{appendix.F}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.1}{\ignorespaces Prior-Contrastive Average KL Divergence\relax }}{95}{figure.caption.60}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.2}{\ignorespaces Prior-Contrastive Estimator Loss\relax }}{95}{figure.caption.61}}
\@writefile{lof}{\contentsline {figure}{\numberline {F.3}{\ignorespaces Prior-Contrastive $NELBO$\relax }}{95}{figure.caption.62}}
\@writefile{toc}{\contentsline {chapter}{\numberline {Appendix G}Second Functional Derivatives of Direct Log Ratio Estimator Losses}{96}{appendix.G}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{app:F}{{G}{96}{Second Functional Derivatives of Direct Log Ratio Estimator Losses}{appendix.G}{}}
