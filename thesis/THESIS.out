\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Problem Context}{section.1}% 2
\BOOKMARK [1][-]{section.2}{Learning}{}% 3
\BOOKMARK [2][-]{subsection.2.1}{Variational Inference}{section.2}% 4
\BOOKMARK [3][-]{subsubsection.2.1.1}{Context}{subsection.2.1}% 5
\BOOKMARK [3][-]{subsubsection.2.1.2}{Introduction to Variational Inference}{subsection.2.1}% 6
\BOOKMARK [3][-]{subsubsection.2.1.3}{Derivation of the ELBO}{subsection.2.1}% 7
\BOOKMARK [3][-]{subsubsection.2.1.4}{Mean-Field Variational Family}{subsection.2.1}% 8
\BOOKMARK [3][-]{subsubsection.2.1.5}{Example: Bayesian mixture of Gaussians}{subsection.2.1}% 9
\BOOKMARK [3][-]{subsubsection.2.1.6}{References}{subsection.2.1}% 10
\BOOKMARK [2][-]{subsection.2.2}{Neural Networks}{section.2}% 11
\BOOKMARK [3][-]{subsubsection.2.2.1}{Motivation}{subsection.2.2}% 12
\BOOKMARK [3][-]{subsubsection.2.2.2}{Neural Network Structure}{subsection.2.2}% 13
\BOOKMARK [3][-]{subsubsection.2.2.3}{Bias-per-node Representation}{subsection.2.2}% 14
\BOOKMARK [3][-]{subsubsection.2.2.4}{Choice of Activation Function}{subsection.2.2}% 15
\BOOKMARK [3][-]{subsubsection.2.2.5}{Optimization}{subsection.2.2}% 16
\BOOKMARK [3][-]{subsubsection.2.2.6}{Back-propogation}{subsection.2.2}% 17
\BOOKMARK [3][-]{subsubsection.2.2.7}{Weight Initialization}{subsection.2.2}% 18
\BOOKMARK [3][-]{subsubsection.2.2.8}{References}{subsection.2.2}% 19
\BOOKMARK [2][-]{subsection.2.3}{Kernel Density Estimation}{section.2}% 20
\BOOKMARK [1][-]{section.3}{Our Problem}{}% 21
\BOOKMARK [2][-]{subsection.3.1}{Objective Derivation}{section.3}% 22
\BOOKMARK [3][-]{subsubsection.3.1.1}{Implicit Prior}{subsection.3.1}% 23
\BOOKMARK [3][-]{subsubsection.3.1.2}{Implicit Likelihood \(\046 Prior\)}{subsection.3.1}% 24
\BOOKMARK [2][-]{subsection.3.2}{Class Probability Estimation}{section.3}% 25
\BOOKMARK [3][-]{subsubsection.3.2.1}{Procedure \(need better heading\)}{subsection.3.2}% 26
\BOOKMARK [3][-]{subsubsection.3.2.2}{Implicit Prior}{subsection.3.2}% 27
\BOOKMARK [3][-]{subsubsection.3.2.3}{Implicit Likelihood \(\046 Prior\)}{subsection.3.2}% 28
\BOOKMARK [3][-]{subsubsection.3.2.4}{Optimal functions}{subsection.3.2}% 29
\BOOKMARK [2][-]{subsection.3.3}{Divergence Minimisation}{section.3}% 30
\BOOKMARK [3][-]{subsubsection.3.3.1}{Procedure \(Again need better heading\)}{subsection.3.3}% 31
\BOOKMARK [3][-]{subsubsection.3.3.2}{Implicit Prior}{subsection.3.3}% 32
\BOOKMARK [3][-]{subsubsection.3.3.3}{Implicit Likelihood \(\046 Prior\)}{subsection.3.3}% 33
\BOOKMARK [3][-]{subsubsection.3.3.4}{Optimal functions}{subsection.3.3}% 34
\BOOKMARK [3][-]{subsubsection.3.3.5}{Alternative Derivation of Class Probability Estimation}{subsection.3.3}% 35
\BOOKMARK [1][-]{section.4}{Experiments}{}% 36
\BOOKMARK [2][-]{subsection.4.1}{"Sprinkler" Example}{section.4}% 37
\BOOKMARK [3][-]{subsubsection.4.1.1}{Problem Context}{subsection.4.1}% 38
\BOOKMARK [3][-]{subsubsection.4.1.2}{Program Structure}{subsection.4.1}% 39
\BOOKMARK [3][-]{subsubsection.4.1.3}{Results}{subsection.4.1}% 40
\BOOKMARK [2][-]{subsection.4.2}{haha why is CPE better than KL min its conspiracy theory time}{section.4}% 41
\BOOKMARK [2][-]{subsection.4.3}{Experiments to prove me theory}{section.4}% 42
\BOOKMARK [2][-]{subsection.4.4}{Autoencoding Variational Bayes \(MNIST image generation\)}{section.4}% 43
\BOOKMARK [3][-]{subsubsection.4.4.1}{Problem Context}{subsection.4.4}% 44
