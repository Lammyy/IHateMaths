\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Problem Context}{section.1}% 2
\BOOKMARK [1][-]{section.2}{Learning}{}% 3
\BOOKMARK [2][-]{subsection.2.1}{Variational Inference}{section.2}% 4
\BOOKMARK [3][-]{subsubsection.2.1.1}{Context}{subsection.2.1}% 5
\BOOKMARK [3][-]{subsubsection.2.1.2}{Introduction to Variational Inference}{subsection.2.1}% 6
\BOOKMARK [3][-]{subsubsection.2.1.3}{Derivation of the ELBO}{subsection.2.1}% 7
\BOOKMARK [3][-]{subsubsection.2.1.4}{Mean-Field Variational Family}{subsection.2.1}% 8
\BOOKMARK [3][-]{subsubsection.2.1.5}{Example: Bayesian mixture of Gaussians}{subsection.2.1}% 9
\BOOKMARK [3][-]{subsubsection.2.1.6}{References}{subsection.2.1}% 10
\BOOKMARK [2][-]{subsection.2.2}{Neural Networks}{section.2}% 11
\BOOKMARK [3][-]{subsubsection.2.2.1}{Motivation}{subsection.2.2}% 12
\BOOKMARK [3][-]{subsubsection.2.2.2}{Neural Network Structure}{subsection.2.2}% 13
\BOOKMARK [3][-]{subsubsection.2.2.3}{Bias-per-node Representation}{subsection.2.2}% 14
\BOOKMARK [3][-]{subsubsection.2.2.4}{Choice of Activation Function}{subsection.2.2}% 15
\BOOKMARK [3][-]{subsubsection.2.2.5}{Optimization}{subsection.2.2}% 16
\BOOKMARK [3][-]{subsubsection.2.2.6}{Back-propogation}{subsection.2.2}% 17
\BOOKMARK [3][-]{subsubsection.2.2.7}{Weight Initialization}{subsection.2.2}% 18
\BOOKMARK [3][-]{subsubsection.2.2.8}{References}{subsection.2.2}% 19
\BOOKMARK [2][-]{subsection.2.3}{Objective Derivation}{section.2}% 20
\BOOKMARK [3][-]{subsubsection.2.3.1}{Implicit Prior}{subsection.2.3}% 21
\BOOKMARK [3][-]{subsubsection.2.3.2}{Implicit Likelihood \(\046 Prior\)}{subsection.2.3}% 22
\BOOKMARK [2][-]{subsection.2.4}{Class Probability Estimation}{section.2}% 23
\BOOKMARK [3][-]{subsubsection.2.4.1}{Procedure \(need better heading\)}{subsection.2.4}% 24
\BOOKMARK [3][-]{subsubsection.2.4.2}{Implicit Prior}{subsection.2.4}% 25
\BOOKMARK [3][-]{subsubsection.2.4.3}{Implicit Likelihood \(\046 Prior\)}{subsection.2.4}% 26
\BOOKMARK [3][-]{subsubsection.2.4.4}{Optimal functions}{subsection.2.4}% 27
\BOOKMARK [2][-]{subsection.2.5}{Divergence Minimisation}{section.2}% 28
\BOOKMARK [3][-]{subsubsection.2.5.1}{Procedure \(Again need better heading\)}{subsection.2.5}% 29
\BOOKMARK [3][-]{subsubsection.2.5.2}{Implicit Prior}{subsection.2.5}% 30
\BOOKMARK [3][-]{subsubsection.2.5.3}{Implicit Likelihood \(\046 Prior\)}{subsection.2.5}% 31
\BOOKMARK [3][-]{subsubsection.2.5.4}{Optimal functions}{subsection.2.5}% 32
\BOOKMARK [3][-]{subsubsection.2.5.5}{Alternative Derivation of Class Probability Estimation}{subsection.2.5}% 33
\BOOKMARK [1][-]{section.3}{Experiments}{}% 34
\BOOKMARK [2][-]{subsection.3.1}{"Sprinkler" Example}{section.3}% 35
\BOOKMARK [3][-]{subsubsection.3.1.1}{Problem Context}{subsection.3.1}% 36
\BOOKMARK [3][-]{subsubsection.3.1.2}{Program Structure}{subsection.3.1}% 37
\BOOKMARK [3][-]{subsubsection.3.1.3}{Results}{subsection.3.1}% 38
\BOOKMARK [2][-]{subsection.3.2}{Autoencoding Variational Bayes \(MNIST image generation\)}{section.3}% 39
\BOOKMARK [3][-]{subsubsection.3.2.1}{Problem Context}{subsection.3.2}% 40
